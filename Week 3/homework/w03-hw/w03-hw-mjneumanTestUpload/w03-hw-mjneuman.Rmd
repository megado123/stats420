---
title: "Week 3 - Homework"
author: "STAT 420, Summer 2017, Megan Masanz, netId: mjneuman"
date: ''
output:
  html_document: 
    toc: yes
  pdf_document: default
---


## Exercise 1 (Using `lm` for Inference)

**(a)** Below is loading the MASS library to access the cats dataset and fitting the dataset to the SLR model with heart weight as the response and body weight as the predictor for for simple linear regression model in `R` and storing the results in a variable called `cat_model` such that:

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

```{r}
library(MASS)

cat_model = lm(Hwt ~ Bwt, data = cats)
```

* For testing for the significance of regression, the The null hypothesis $H_0$ and alternative hypotheses $H_1$ is stated as: 
    + $H_0: \beta_1 = 0$ vs $H_1: \beta_1 \neq 0$
    + Under $H_0$ - there is not a significant linear relationship between a cat's body weight and a cat's heart weight.
    + Under $H_1$ - there is a significant linear relationship between a cat's body weight and a cat's heart weight.

* The value of the test statistic is: 
    + `16.11939`

```{r}
summary(cat_model)$coefficients[2,3]
```

- The p-value of the test:
    + `6.969045e-34`

```{r}
summary(cat_model)$coefficients[2,4]
```

- A statistical decision at $\alpha = 0.01$
    + **Reject the null hypthesis**

- A conclusion in the context of the problem
    + There is a significant linear relationship between a cat's body weight and a cat's heart weight

**(b)** For a 99% confidence interval, $\beta_1$ would be in the range of ``r confint(cat_model, level = 0.99)[2,1]`` for the lower bound and ``r confint(cat_model, level = 0.99)[2,2]`` for the upper bound.  We are 99% confident that the true change in mean cat's heart weight for an increase in cat's body weight of 1 kg is between ``r confint(cat_model, level = 0.99)[2,1]`` and ``r confint(cat_model, level = 0.99)[2,2]``

```{r}
confint(cat_model, level = 0.99)[2, ]
```

**(c)** For a 90% confidence interval, $\beta_0$ would be in the range of of ``r confint(cat_model, level = 0.90)[1,1]`` for the lower bound, and ``r confint(cat_model, level = 0.90)[1,2]`` for the upper bound.  We are 90% confident that for a mean of 0 for a cat's body weight, the cat's heart weight woudl be between ``r confint(cat_model, level = 0.90)[1,1]`` and ``r confint(cat_model, level = 0.90)[1,2]`` 

```{r}
confint(cat_model, level = 0.90)[1, ]
```

**(d)** Using a 95% confidence interval to estimate the mean heart weight for body of of 2.5 kg is `9.728494` grams and for 3.0 kg is `11.745526` grams shown below.  

```{r}
new_bwt= data.frame(Bwt = c(2.5, 3.0))
(predict_data_mean = predict(cat_model, newdata = new_bwt, interval = c("confidence"), level = 0.95))
predict_data_mean[1,1] #mean estimate for 2.5
predict_data_mean[2,1] #mean estimate for 3.0
```

**(d)** The interval that is wider is for the value at `3.0` kg since 3.0 is farther away from the mean of the cats body weight in the dataset than the value of 2.5 is away from the mean of the cats body weight in the dataset.  As you go farther away from the mean value of the cat's body weight, the wider the prediction range becomes.

```{r}
#range for 2.5 is less than range for 3.0
predict_data_mean[1,3]- predict_data_mean[1,2]  < predict_data_mean[2,3]- predict_data_mean[2,2]


#3.0 is farther away from the mean
abs(mean(cats$Bwt) - 3.0) > abs(mean(cats$Bwt) - 2.5)
```

**(e)** Using a 95% prediction interval to predict the heart weight in grams for a body weight of 2.5 kg, the value is `9.728494` and for a body weight of 4.0 kg, the value is `15.779588`

```{r}
new_bwt= data.frame(Bwt = c(2.5, 4.0))
(predict_data_mean = predict(cat_model, newdata = new_bwt, interval = c("prediction"), level = 0.95))
predict_data_mean[1,1] # mean estimate for 2.5
predict_data_mean[2,1] # mean estimate for 4.0
```

**(f)** Create a scatterplot of the data. Add the regression line, 95% confidence bands, and 95% prediction bands.

```{r}
bwt_grid = seq(min(cats$Bwt), max(cats$Bwt), by = 0.01)

hwt_ci_band = predict(cat_model, 
                       newdata = data.frame(Bwt = bwt_grid), 
                       interval = "confidence", level = 0.95)

hwt_pi_band = predict(cat_model, 
                       newdata = data.frame(Bwt = bwt_grid), 
                       interval = "prediction", level = 0.99) 

plot(Hwt ~ Bwt, data = cats,
     xlab = "Body Weight (in Kg)",
     ylab = "Heart weight (in grams)",
     main = "Cats Heart Weight vs Body Weight",
     pch  = 20,
     cex  = 2,
     col  = "grey",
     ylim = c(min(hwt_pi_band), max(hwt_pi_band)))

#Regression line
abline(cat_model, lwd = 5, col = "darkorange")

lines(bwt_grid, hwt_ci_band[,"lwr"], col = "dodgerblue", lwd = 3, lty = 2)
lines(bwt_grid, hwt_ci_band[,"upr"], col = "dodgerblue", lwd = 3, lty = 2)
lines(bwt_grid, hwt_pi_band[,"lwr"], col = "dodgerblue", lwd = 3, lty = 3)
lines(bwt_grid, hwt_pi_band[,"upr"], col = "dodgerblue", lwd = 3, lty = 3)
```

## Exercise 2 (Using `lm` for Inference)

For this exercise we will use the `diabetes` dataset, which can be found in the `faraway` package.

**(a)** Below is loading the faraway library to access the `diabetes` dataset and fitting the dataset to the SLR model with total cholestrol `chol` as the response and weight `weight` as the predictor for the simple linear regression model in `R` and storing the results in a variable called `cholesterol_model` such that:

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

```{r}
library(faraway)
cholesterol_model = lm(chol ~ weight, data = diabetes)
```

* For testing for the significance of regression, the The null hypothesis $H_0$ and alternative hypotheses $H_1$ is stated as: 
    + $H_0: \beta_1 = 0$ vs $H_1: \beta_1 \neq 0$
    + Under $H_0$ there is not a significant linear relationship between total cholestrol and weight.
    + Under $H_1$ there is a significant linear relationship between total cholestrol and weight.

* The value of the test statistic is (retrieved below): 
    + 1.339096

```{r}
summary(cholesterol_model)$coefficients[2,3]
```

* The p-value of the test (retrieved below):
    + 0.1813018

```{r}
summary(cholesterol_model)$coefficients[2,4]
```

* The statistical decision at $\alpha = 0.05$ is:  **fail to reject the null hypothesis**

* The conclusion we can draw is there is no significant linear relation between weight and cholestrol.


**(b)** Fit the following simple linear regression model in `R`. Use HDL as the response and weight as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

```{r}
library(faraway)
hdl_model = lm(hdl ~ weight, data = diabetes)
```


* For testing for the significance of regression, the The null hypothesis $H_0$ and alternative hypotheses $H_1$ is stated as: 
    + $H_0: \beta_1 = 0$ vs $H_1: \beta_1 \neq 0$
    + Under $H_0$ there is not a significant linear relationship between hdl and weight.
    + Under $H_1$ there is a significant linear relationship between hdl and weight.

* The value of the test statistic is (retrieved below): 
    + -6.075

```{r}
summary(hdl_model)$coefficients[2,3]
```

* The p-value of the test (retrieved below):
    + 2.890526e-09

```{r}
summary(hdl_model)$coefficients[2,4]
```

* The statistical decision at $\alpha = 0.05$ is:  **reject the null hypothesis**

* The conclusion we can draw is there is a significant linear relation between hdl and weight.

## Exercise 3 (Inference "without" `lm`)

The function named `get_p_val_beta_1` performs the test

$$
H_0: \beta_1 = \beta_{10} \quad \text{vs} \quad H_1: \beta_1 \neq \beta_{10}
$$

for the linear model

$$
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i.
$$

The function should take two inputs:

- A model object that is the result of fitting the SLR model with `lm()`
- A hypothesized value of $\beta_1$, $\beta_{10}$, with a default value of 0

The function should return a named vector with elements:

- `t`, which stores the value of the test statistic for performing the test
- `p_val`, which stores the p-value for performing the test

```{r}
get_p_val_beta_1 = function(model, beta_1_0 = 0) {
  model_info    = summary(model)$coefficients
  beta_1_hat    = model_info[2,1]    #estimate
  beta_1_hat_se = model_info[2,2]    # std error
  t = (beta_1_hat - beta_1_0)/ beta_1_hat_se
  p_val = 2 * pt(abs(t), df = length(resid(model)) - 2, lower.tail = FALSE)
  values = c(
    t = t,
    p_val = p_val)
}
```

**(a)** After writing the function, run these three lines of code:

```{r}
(get_p_val_beta_1(cat_model, beta_1 = 4.2))
(get_p_val_beta_1(cholesterol_model))
(get_p_val_beta_1(hdl_model))
```

**(b)** Return to the goalies dataset from the previous homework, which is stored in [`goalies.csv`](goalies.csv). Fit a simple linear regression model with `W` as the response and `MIN` as the predictor. Store the results in a variable called `goalies_model_min`. After doing so, run these three lines of code:

```{r}
library(readr)
goalies = read_csv("goalies.csv")
goalies_model_min = lm(W ~ MIN, data = goalies)
```

```{r, eval = TRUE}
(get_p_val_beta_1(goalies_model_min))
(get_p_val_beta_1(goalies_model_min, beta_1 = coef(goalies_model_min)[2]))
(get_p_val_beta_1(goalies_model_min, beta_1 = 0.008))
```

## Exercise 4 (Simulating Sampling Distributions)

For this exercise we will simulate data from the following model:

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Where $\epsilon_i \sim N(0, \sigma^2).$ Also, the parameters are known to be:

- $\beta_0 = 3$
- $\beta_1 = 0.75$
- $\sigma^2 = 25$

We will use samples of size $n = 42$.

**(a)** Below is simulating this model $1500$ times. Each time use `lm()` to fit a simple linear regression model, then stored the value of $\hat{\beta}_0$ and $\hat{\beta}_1$. Set a seed using **my** birthday before performing the simulation. Note, we are simulating the $x$ values once, and then they remain fixed for the remainder of the exercise.

```{r}
birthday = 19810803
set.seed(birthday)
n = 42
x = seq(0, 20, length = n)

sigma  = 5
beta_0 = 3
beta_1 = 0.75
sim_beta_hat = function(x, beta_0, beta_1, sigma) {
  beta_hat_0 = rep(0, 1500)
  beta_hat_1 = rep(0, 1500)

  for (i in 1:1500){
    epsilon = rnorm(n, mean = 0, sd = sigma)
    y = beta_0 + beta_1 * x + epsilon
    sim_data = data.frame(predictor = x, response = y)
    sim_model = lm(response ~ predictor, data = sim_data)
    beta_hat_0[i] = coef(sim_model)[1]
    beta_hat_1[i] = coef(sim_model)[2]
  }
  data.frame(beta_hat_0 = beta_hat_0, beta_hat_1 = beta_hat_1)
}

beta_hat = sim_beta_hat(x, beta_0, beta_1, sigma)
```

**(b)** For the *known* values of $x$, the expected value of $\hat{\beta}_1$ is **0.75** given $E[\hat{\beta}_1] = \beta_1$ since $\hat{\beta}_1 \sim N(  0.75, 0.01702658)$ so $E[\hat{\beta}_1] = 0.75$

Below is retriving the variance for display purposes (and used in later calculation.)

```{r}
Sxx = sum((x - mean(x)) ^ 2)
(var_beta_1_hat = sigma ^ 2 / Sxx)
```

**(c)**The standard deviation of $\hat{\beta_1}$ is **0.1304859**

```{r}
(sd_beta_hat_1 = sigma/(sqrt(Sxx)))
```

**(d)** The mean of your simulated values of $\hat{\beta}_1$ is **0.7445087**.  Yes this does make sense, it is very close to the expected value.

```{r}
mean(beta_hat$beta_hat_1)

```

**(e)**The standard deviation of the simulated values of $\hat{\beta}_1$ is **0.1300722**.  This is close to the standard deviation of $\hat{\beta_1}$ which does make sense. 

```{r}
sd(beta_hat$beta_hat_1)
```

**(f)** For the known values of $x$, the expected value of $\hat{\beta}_0$ is **3** given $E[\hat{\beta}_0] = \beta_0$

**(g)** For the known values of $x$,  the standard deviation of $\hat{\beta}_0$ is **1.555653**

```{r}
(sd_beta_hat_0 = sigma * sqrt( (1/n) + (mean(x) ^ 2 / Sxx)) )
```

**(h)** The mean of your simulated values of $\hat{\beta}_0$ is **3.09209**, given the answer in f, yes this does make sense since it is pretty close to teh expected value.

```{r}
mean(beta_hat$beta_hat_0)
```

**(i)** The standard deviation of your simulated values of $\hat{\beta}_0$ is **1.510165** given the answer in g, yes this does make sense, this value is again very close to the standard deviation found in g.

```{r}
sd(beta_hat$beta_hat_0)
```

**(j)** Below is a plot of a histogram of the simulated values for $\hat{\beta}_1$. Added is the normal curve for the true sampling distribution of $\hat{\beta}_1$.

```{r}
hist(beta_hat$beta_hat_1, prob = TRUE, breaks = 20,
     xlab = expression(hat(beta)[1]), main = "", border = "dodgerblue")

curve(dnorm(x, mean = beta_1, sd = sd_beta_hat_1),
      col = "darkorange", add = TRUE, lwd = 3)

```

**(k)** Below is a plot of a histogram of the simulated values for $\hat{\beta}_0$. Added is the normal curve for the true sampling distribution of $\hat{\beta}_0$.

```{r}
hist(beta_hat$beta_hat_0, prob = TRUE, breaks = 20,
     xlab = expression(hat(beta)[0]), main = "", border = "dodgerblue")

curve(dnorm(x, mean = beta_0, sd = sd_beta_hat_0),
      col = "darkorange", add = TRUE, lwd = 3)
```

## Exercise 5 (Simulating Confidence Intervals)

For this exercise we will simulate data from the following model:

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Where $\epsilon_i \sim N(0, \sigma^2).$ Also, the parameters are known to be:

- $\beta_0 = 1$
- $\beta_1 = 3$
- $\sigma^2 = 16$

We will use samples of size $n = 20$.

Our goal here is to use simulation to verify that the confidence intervals really do have their stated confidence level. Do **not** use the `confint()` function for this entire exercise.

**(a)** Simulate this model $2000$ times. Each time use `lm()` to fit a simple linear regression model, then store the value of $\hat{\beta}_0$ and $s_e$. Set a seed using **your** birthday before performing the simulation. Note, we are simulating the $x$ values once, and then they remain fixed for the remainder of the exercise.

```{r}
birthday = 19810803
set.seed(birthday)
n = 20
x = seq(-5, 5, length = n)

sigma  = 4
beta_0 = 1
beta_1 = 3

sim_data = function(x, beta_0, beta_1, sigma) {
  beta_hat_0 = rep(0, 2000)
  s_e = rep(0, 2000)
  for (i in 1:2000){
    epsilon = rnorm(n, mean = 0, sd = sigma)
    y = beta_0 + beta_1 * x + epsilon
    sim_data = data.frame(predictor = x, response = y)
    sim_model = lm(response ~ predictor, data = sim_data)
    beta_hat_0[i] = coef(sim_model)[1]
    s_e = summary(sim_model)$sigma
  }
  data.frame(beta_hat_0 = beta_hat_0, s_e = s_e)
}
vals  = sim_data(x, beta_0, beta_1, sigma)

```

**(b)** For each of the $\hat{\beta}_0$ that you simulated, calculate a 90% confidence interval. Store the lower limits in a vector `lower_90` and the upper limits in a vector `upper_90`. Some hints:

- You will need to use `qt()` to calculate the critical value, which will be the same for each interval.
- Remember that `x` is fixed, so $S_{xx}$ will be the same for each interval.
- You could, but do not need to write a `for` loop. Remember vectorized operations.

```{r}
crit          = qt(0.95, df = nrow(vals) - 2)    #critical value, changes with alpha value
Sxx           = sum((x - mean(x)) ^ 2)           #Sxx, stays the same for each interval
SE_beta_0_hat = vals$s_e * sqrt((1/n) + ((mean(x)^2) / Sxx))

lower_90 = vals$beta_hat_0 - (crit * SE_beta_0_hat)
upper_90 = vals$beta_hat_0 + (crit * SE_beta_0_hat)
```

**(c)**The proportion of these intervals that contain the true value of $\beta_0$ is `0.8655`

```{r}
mean(beta_0 > lower_90 & beta_0 < upper_90)
```

**(d)** Based on these intervals, the proportion of the simulations that would reject the test $H_0: \beta_0 = 0$ vs $H_1: \beta_0 \neq 0$ at $\alpha = 0.10$ is `0.3705`

```{r}
1 - (mean(0 > lower_90 & 0 < upper_90))
```

**(e)** For each of the $\hat{\beta}_0$ that you simulated, calculate a 99% confidence interval. Store the lower limits in a vector `lower_99` and the upper limits in a vector `upper_99`.

```{r}
crit          = qt(0.995, df = nrow(vals) - 2)   #critical value

lower_99 = vals$beta_hat_0 - (crit * SE_beta_0_hat)
upper_99 = vals$beta_hat_0 + (crit * SE_beta_0_hat)
```

**(f)** The proportion of these intervals contain the true value of $\beta_0$ is `0.9855`

```{r}
mean(beta_0 > lower_99 & beta_0 < upper_99)
```

**(g)** Based on these intervals, the proportion of the simulations that would reject the test $H_0: \beta_0 = 0$ vs $H_1: \beta_0 \neq 0$ at $\alpha = 0.01$ is `0.121`

```{r}
1 - (mean(0 > lower_99 & 0 < upper_99))
```

