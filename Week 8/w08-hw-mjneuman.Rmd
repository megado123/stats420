---
title: "Week 8 - Homework"
author: "STAT 420, Summer 2017, Megan Masanz, mjneuman"
date: ''
output:
  html_document: 
    toc: yes
  pdf_document: default
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80)
```

## Exercise 1 (Writing Functions)

**(a)** Write a function named `diagnostics` that takes as input the arguments:

- `model`, an object of class `lm(), that is a model fit via `lm()`
- `pcol`, for controlling point colors in plots, with a default value of `black`
- `lcol`, for controlling line colors in plots, with a default value of `white`
- `alpha`, the significance level of any test that will be performed inside the function, with a default value of `0.05`
- `plotit`, a logical value for controlling display of plots with default value `TRUE`
- `testit`, a logical value for controlling outputting the results of tests with default value `TRUE`

The function should output:

- A list with two elements when `testit` is `TRUE`:
    - `p_val`, the p-value for the Shapiro-Wilk test for assesing normality
    - `decision`, the decision made when performing the Shapiro-Wilk test using the `alpha` value input to the function. "Reject" if the null hypothesis is rejected, otherwise "Fail to Reject".
- Two plots, side-by-side, when `plotit` is `TRUE`:
    - A fitted versus residuals plot that adds a horizontal line at $y = 0$, and labels the $x$-axis "Fitted" and the $y$-axis "Residuals". The points and line should be colored according to the input arguments. Give the plot a title. 
    - A Normal Q-Q plot of the residuals that adds the appropriate line using `qqline()`. The points and line should be colored according to the input arguments. Be sure the plot has a title. 
    
    
```{r}
diagnostics = function(model, pcol= 1, lcol = 0, alpha = 0.05, plotit = TRUE, testit = TRUE){
  if (plotit == TRUE){
    par(mfrow = c(1, 2))
    plot(fitted(model), resid(model), col = pcol, pch = 20,
         xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
    abline(h = 0, col = lcol, lwd = 2)
    qqnorm(resid(model), main = "Normal Q-Q Plot", col = pcol)
    qqline(resid(model), col = lcol, lwd = 2)
  }
  if (testit == TRUE){
    p_val = shapiro.test(resid(model))$p.value
    decision = ifelse(p_val < alpha, "Reject", "Fail to Reject")
    return_val = list(p_val = p_val, decision = decision)
    return_val
  }
}
```

**(b)** Run the following code.

```{r, eval = TRUE}
set.seed(42)
data1 = data.frame(x = runif(n = 20, min = 0, max = 10),
                   y = rep(x = 0, times = 20))
data1$y = with(data1, 5 + 2 * x + rnorm(n = 20))
fit1 = lm(y ~ x, data = data1)

data2 = data.frame(x = runif(n = 30, min = 0, max = 10),
                   y = rep(x = 0, times = 30))
data2$y = with(data2, 2 + 1 * x + rexp(n = 30))
fit2 = lm(y ~ x, data = data2)

data3 = data.frame(x = runif(n = 40, min = 0, max = 10),
                   y = rep(x = 0, times = 40))
data3$y = with(data3, 2 + 1 * x + rnorm(n = 40, sd = x))
fit3 = lm(y ~ x, data = data3)

diagnostics(fit1, plotit = FALSE)$p_val
diagnostics(fit1, testit = FALSE, pcol = "darkorange", lcol = "dodgerblue")

diagnostics(fit2, plotit = FALSE)$decision
diagnostics(fit2, testit = FALSE, pcol = "grey", lcol = "green")

diagnostics(fit3)
```

## Exercise 2 (Swiss Fertility Data)

For this exercise, we will use the `swiss` data, which can be found in the `faraway` package. After loading the `faraway` package, use `?swiss` to learn about this dataset.

```{r, message = FALSE, warning = FALSE}
library(faraway)
```

**(a)** Fit an additive multiple regression model with `Fertility` as the response and the remaining variables in the `swiss` dataset as predictors. Report the $R^2$ for this model.


```{r}
model_swiss_add = lm(Fertility ~ Agriculture + Examination + Education + Catholic + Infant.Mortality, data = swiss)

summary(model_swiss_add)$r.squared

```

The $R^2$ for this model is: **`r summary(model_swiss_add)$r.squared`**


**(b)** Check the constant variance assumption for this model. Do you feel it has been violated? Justify your answer.

First in checking for constant variance we create a Fitted vs Residuals Plot

```{r}
plot(fitted(model_swiss_add), resid(model_swiss_add), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals Plot swiss Additive Model")
abline(h = 0, col = "darkorange", lwd = 2)
```

For the fitted values, the spread of the residuals appears to be roughly the same implying constant variance so we continue with looking at a Breusch-Pagan Test to check for constant variance as well.

```{r, message = FALSE, warning = FALSE}
library(lmtest)
bptest(model_swiss_add)
```

Given the high p-value, so we fail to reject the null hypothesis and say that it appears that there is constant variance, so **it appears the constant variance has not been violated**

**(c)** Check the normality assumption for this model. Do you feel it has been violated? Justify your answer.

Using a Q-Q plot to investigate normality, it appears the errors could be normally distributed

```{r}
qqnorm(resid(model_swiss_add), main = "Normal Q-Q Plot, model_swiss_add", col = "darkgrey")
qqline(resid(model_swiss_add), col = "dodgerblue", lwd = 2)
```

The Q-Q plot shown above seems to suggest that the data follows a normal distribution.  Finally using the Shapiro test we can see a high p-value which suggests that this data **does not violate the normality assumption**.

```{r}
shapiro.test(resid(model_swiss_add))
```


**(d)** Check for any high leverage observations. Report any observations you determine to have high leverage.

```{r}
#number of large leverages
sum(hatvalues(model_swiss_add) > 2 * mean(hatvalues(model_swiss_add)))

```

There appear to be 2 points with high leverage

```{r}
#retrieve index of large leverages - there should be 2
which(hatvalues(model_swiss_add) > 2 * mean(hatvalues(model_swiss_add)))
```


**(e)** Check for any influential observations. Report any observations you determine to be influential.

Looking for points with high cooks distance to identify points with a high influence.

```{r}
infl_value = 4/length(cooks.distance(model_swiss_add))

#points with high influence
names(which(cooks.distance(model_swiss_add) > infl_value))

#indexes of points with high influence
indexes = unname(which(cooks.distance(model_swiss_add) > infl_value))
```

The country observation points with high influence include:  **`r names(which(cooks.distance(model_swiss_add) > infl_value)) `**

**(f)** Refit the additive multiple regression model without any points you identified as influential. Compare the coefficients of this fitted model to the previously fitted model.

```{r}
subset_data = swiss[-indexes ,]

model_swiss_add_subset  = lm(Fertility ~ Agriculture + Examination + Education 
                             + Catholic + Infant.Mortality, data = subset_data)

coef(model_swiss_add_subset)

coef(model_swiss_add)


compare = rbind(c("beta_0_hat", coef(model_swiss_add)[1], coef(model_swiss_add_subset)[1],                      coef(model_swiss_add)[1] - coef(model_swiss_add_subset)[1] ),
                
                c("beta_1_hat", coef(model_swiss_add)[2], coef(model_swiss_add_subset)[2],                      coef(model_swiss_add)[2] - coef(model_swiss_add_subset)[2] ),
                
                c("beta_2_hat", coef(model_swiss_add)[3], coef(model_swiss_add_subset)[3],                      coef(model_swiss_add)[3] - coef(model_swiss_add_subset)[3] ),
                
                c("beta_3_hat", coef(model_swiss_add)[4], coef(model_swiss_add_subset)[4],                      coef(model_swiss_add)[4] - coef(model_swiss_add_subset)[4] ),
                
                c("beta_4_hat", coef(model_swiss_add)[5], coef(model_swiss_add_subset)[5],                      coef(model_swiss_add)[5] - coef(model_swiss_add_subset)[5] ),
                
                c("beta_5_hat", coef(model_swiss_add)[6], coef(model_swiss_add_subset)[6],                      coef(model_swiss_add)[6] - coef(model_swiss_add_subset)[6] )
                )

colnames(compare) = c("Beta Parameter", "Model Parm Value for All data", "Model Parm value for Subset of Data", "Beta from All Minus Beta from Subset")
knitr::kable(compare, format = "markdown", padding = 3)

```

Removing the points from the model, doesn't seem to make too much of an impact on the coeffiencients.  The largest difference was on the intercept $\hat{\beta_0}$, subsetting the data decreased it by `r coef(model_swiss_add)[1] - coef(model_swiss_add_subset)[1] `

**(g)** Create a data frame that stores the observations that were "removed" because they were influential. Use the two models you have fit to make predictions with these observations. Comment on the difference between these two sets of predictions.

```{r}
removed_data = swiss[indexes, ]

removed_data$Fertility
predict(model_swiss_add, removed_data)
predict(model_swiss_add_subset, removed_data)
```

It is interesting that for most points, when using the model created with the subset of data, the points are generally higher, but that is not the case for Sierre.  

## Exercise 3 (Why Bother?)

**Why** do we care about violations of assumptions? One key reason is that the distributions of the parameters that we have used are all reliant on these assumptions. When the assumptions are violated, the distributional results are not correct, so our tests are garbage. **Garbage In, Garbage Out!**

Consider the following setup that we will use for the remainder of the exercise. We choose a sample size of 100.

```{r}
n = 100
set.seed(42)
x_1 = runif(n, -2, 2)
x_2 = runif(n, 0, 5)
```

Consider the model,

\[
Y = 5 + 0 x_1 + 1 x_2 + \epsilon.
\]

That is,

- $\beta_0$ = 5
- $\beta_1$ = 0
- $\beta_2$ = 1

We now simulate `y_1` in a manner that does not violate any assumptions, which we will verify. In this case $\epsilon \sim N(0, 1).$

```{r}
set.seed(420)
y_1 = 5 + 0 * x_1 + 1 * x_2 + rnorm(n = n, mean = 0, sd = 1)
fit_1 = lm(y_1 ~ x_1 + x_2)
qqnorm(resid(fit_1), col = "dodgerblue")
qqline(resid(fit_1), col = "darkorange", lwd = 2)
shapiro.test(resid(fit_1))
```

Then, we simulate `y_2` in a manner that **does** violate assumptions, which we again verify. In this case $\epsilon \sim N(0, \sigma = |x_1|).$

```{r}
set.seed(42)
y_2 = 5 + 0 * x_1 + 1 * x_2  + rnorm(n = n, mean = 0, sd = abs(x_1))
fit_2 = lm(y_2 ~ x_1 + x_2)
qqnorm(resid(fit_2), col = "dodgerblue")
qqline(resid(fit_2), col = "darkorange", lwd = 2)
shapiro.test(resid(fit_2))
```

**(a)** Use the following code after changing `birthday` to your birthday.

Repeat the  process of generating `y_1` and `y_2` as defined, and fit models with each as the response `2500` times. Each time, store the p-value for testing,

\[
\beta_1 = 0,
\]

using both models, in the appropriate variables defined above. (You do not need to use a data frame as we have in the past. Although, feel free to modify the code to instead use a data frame.)

```{r}
n = 100
set.seed(42)
x_1 = runif(n, -2, 2)
x_2 = runif(n, 0, 5)

birthday = 19810803
set.seed(birthday)

sim = function(){
  num_sims = 2500
  p_val_1 = rep(0, num_sims)
  p_val_2 = rep(0, num_sims)
  
  for (i in 1: num_sims){
    y_1 = 5 + 0 * x_1 + 1 * x_2 + rnorm(n = n, mean = 0, sd = 1)
    fit_1 = lm(y_1 ~ x_1 + x_2)
    p_val_1[i] = coef(summary(fit_1))[2,"Pr(>|t|)"]
  
    #violates assumptions
    y_2 = 5 + 0 * x_1 + 1 * x_2  + rnorm(n = n, mean = 0, sd = abs(x_1))
    fit_2 = lm(y_2 ~ x_1 + x_2)
    p_val_2[i] = coef(summary(fit_2))[2,"Pr(>|t|)"]
  }
  data.frame(p_val_1 = p_val_1, p_val_2 = p_val_2)
}

results = sim()

```

**(b)** What proportion of the `p_val_1` values are less than 0.01? Less than 0.05? Less than 0.10? What proportion of the `p_val_2` values are less than 0.01? Less than 0.05? Less than 0.10? Arrange your results in a table. Briefly explain these results.

```{r}
result = rbind(c("Model 1, p-value proportion less than 0.01", mean(results$p_val_1 < 0.01)),
               c("Model 1, p-value proportion less than 0.05", mean(results$p_val_1 < 0.05)),
               c("Model 1, p-value proportion less than 0.10", mean(results$p_val_1 < 0.10)),
               c("Model 2, p-value proportion less than 0.01", mean(results$p_val_2 < 0.01)),
               c("Model 2, p-value proportion less than 0.05", mean(results$p_val_2 < 0.05)),
               c("Model 2, p-value proportion less than 0.10", mean(results$p_val_2 < 0.10)))

colnames(result) = c("Description", "proportion")
knitr::kable(result, format = "markdown", padding = 3)
```

We know that Model 1 does not violate are assumptions regarding normality, so for Model 1, for a given p-value we would expect to see $\alpha$ be the number of times that we have a Type 1 error, this is shown in the proportion column of the table which fairly accurately shows the correct information.

For Model 2 this means that we see a much higher number of Type 1 errors, which its value we know we cannot rely on since `garbage in, garbage out`.  In this case the values indicate there is a type 1 error more often than in the model that does not violate assumptions, but this is not actually a conclusion, we know that this value is not reliable if model assumptions are not accurate.

## Exercise 4 (TV Is Healthy?)

For this exercise, we will use the `tvdoctor` data, which can be found in the `faraway` package. After loading the `faraway` package, use `?tvdoctor` to learn about this dataset.

```{r, message = FALSE, warning = FALSE}
library(faraway)
```

**(a)** Fit a simple linear regression with `life` as the response and `tv` as the predictor. Plot a scatterplot and add the fitted line. Check the assumptions of this model.

```{r}
model_tv = lm(life ~ tv, data = tvdoctor)

plot(life ~ tv, data = tvdoctor, col = "grey", pch = 20,
     main = "Data from SLR Model, Life vs TV")
fit_1 = lm(life ~ tv, data = tvdoctor)
abline(fit_1, col = "darkorange", lwd = 3)
```

Checking the assumptions of this model, plotting the Fitted vs Residuals for the SLR model

```{r}
plot(fitted(model_tv), resid(model_tv), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals from SLR Model")
abline(h = 0, col = "darkorange", lwd = 2)
```


**It does appear that the mean of the residuals is not centered at 0.**  This would imply that the linearity assumption is invalid.  However, we do see a clear pattern in this data suggesting that a high-order polynomial maybe the solution.

**It does not appear that the spread of the residuals is the same.** This implies that constant variance of the errors is suspect, but this maybe a result of a bad model (garbage in, garbage out).  Further we see a high p-value of `r bptest(model_tv)$p.value` which would indicate that we don't reject the null hypothesis, so we can say that it appears that constant variance is not violated.


```{r}
bptest(model_tv)
#we see a high p-value so we don't reject the null hypothesis
```

To look at the normality of the errors, we can further we can look at a Q-Q Plot

```{r}
qqnorm(resid(model_tv), main = "Normal Q-Q Plot, SLR model tv", col = "darkgrey")
qqline(resid(model_tv), col = "dodgerblue", lwd = 2)
```

which does have some tails, but it doesn't look like this is clearly coming from something that is violating the normality assumption.  Further looking at the Shapiro-Wilk test we see a high p-value indicating the normality is not suspect.

```{r}
shapiro.test(resid(model_tv))
```

**(b)** Fit higher order polynomial models of degree 3, 5, and 7. For each, plot a fitted versus residuals plot and comment on the constant variance assumption. Based on those plots, which of these three models do you think are acceptable? Use a statistical test(s) to compare the models you just chose. Based on the test, which is preferred? Check the normality assumption of this model. Identify any influential observations of this model.

Based on the `Residuals vs Fitted` Plots, the best model appears to be the polynomial model of degree 5.  We can see in the plot below, that it is over-fitting the model, but based on `Fitted vs Residual` Plots, this appears to be the acceptable model.

```{r}
#setting up models
model_tv       = lm(life~ tv, data = tvdoctor)
model_tv_poly3 = lm(life ~ tv + I(tv ^ 2) +  I(tv ^ 3), data = tvdoctor)
model_tv_poly5 = lm(life ~ tv + I(tv ^ 2) +  I(tv ^ 3) + I(tv ^ 4) + I(tv ^ 5), 
                    data = tvdoctor)
model_tv_poly7 = lm(life ~ tv + I(tv ^ 2) +  I(tv ^ 3) + I(tv ^ 4) + I(tv ^ 5) 
                    + I(tv ^ 6) + I(tv ^ 7), data = tvdoctor)

#function for plotting
plot_tv_curve = function(model){
  plot(life ~ tv, data = tvdoctor, xlab = "Number of people per tv set", 
       ylab = "Life expectancy in years", col = "dodgerblue", 
       pch = 20, cex =2)
  xplot = seq(0, 600, by = 100)
  lines(xplot, predict(model, newdata = data.frame(tv = xplot)),
        col = "darkorange", lwd = 2, lty = 1)
}

```

```{r}
#polynomial 3
par(mfrow = c(1, 2))
plot_tv_curve(model_tv_poly3)

plot(fitted(model_tv_poly3), resid(model_tv_poly3), xlab = "Fitted", ylab = "Residuals", 
     col = "dodgerblue", pch = 20, cex =2)
abline(h = 0, col = "darkorange", lwd = 2)

shapiro.test(resid(model_tv_poly3))$p.value #high pvalue, we won't reject normality

bptest(model_tv_poly3)$p.value # high pvalue, won't reject constant variance

#There are high p-values here for both the BPTest and the Shapiro-Wilkens test, 
#however the fitted vs residuals plot does not have the best trend
```

In the polynomial of degree 3, the `Fitted vs Residuals` plot looks a lot better than the `Fitted vs Residual` Plot of the SLR model, but there still appears to be a pattern suggesting a higher polynomial.  The high p-value for the Shapiro-Wilk test suggests that normality is not suspect, and the high p-value for the bptest suggests that we won't reject constant variance, but we will defer to the plot and look at a higher degree polynomial.

```{r}
#polynomial 5
par(mfrow = c(1, 2))
plot_tv_curve(model_tv_poly5)


plot(fitted(model_tv_poly5), resid(model_tv_poly5), xlab = "Fitted", ylab = "Residuals", 
     col = "dodgerblue", pch = 20, cex =2)
abline(h = 0, col = "darkorange", lwd = 2)

shapiro.test(resid(model_tv_poly5))$p.value #high pvalue, we won't reject constant variance

bptest(model_tv_poly5)$p.value # high pvalue, won't reject constant variance
```

In the case of the polynomial of degree 5, we can clearly see that it is overfitting the model, however the `Fitted vs Residual` Plot much better than that of the polynomial of degree 3, so based on this plot, I would choose this over the polynomial of degree 3.  The high p-value for the Shapiro-Wilk test suggests that normality is not suspect, and the high p-value for the bptest suggests that we won't reject constant variance.  There is not longer a clear pattern in the `Fitted vs Residual` Plot, and knowing that simplier models are better, this model actually is our selected model.

Finally, exploring the polynomial of degree 7

```{r}
#polynomial 7
par(mfrow = c(1, 2))
plot_tv_curve(model_tv_poly7)

plot(fitted(model_tv_poly7), resid(model_tv_poly7), xlab = "Fitted", ylab = "Residuals", 
     col = "dodgerblue", pch = 20, cex =2)
abline(h = 0, col = "darkorange", lwd = 2)

shapiro.test(resid(model_tv_poly7))$p.value #high pvalue, we won't reject constant variance

bptest(model_tv_poly7)$p.value # high pvalue, won't reject constant variance
```

In the case of the polynomial of degree 7, we can clearly see that it is overfitting the model, however the `Fitted vs Residual` Plot looks about the same as that of the polynomial of degree 5, so based on this plot, I would choose the polynomial of degree 5.  The high p-value for the Shapiro-Wilk test suggests that normality is not suspect, and the high p-value for the bptest suggests that we won't reject constant variance.  There is not a clear pattern in the `Fitted vs Residual` Plot, and knowing that simplier models are better, we would choose the polynomial of degree 5.


Further doing an anova F-test on the models, since they are nested since hierachy was retained, we can see that while the model with a polynomial degree of 3 was preferred over the simple SLR model, the ploynomial degree 5 was preferred over the polymonial of degree 3, and further the polynomial of degree 7 was not preferred over the polynomial of degree 5, so the 5th degree polynomial is the preferred model.

```{r}
#Check with statistical tests
anova(model_tv, model_tv_poly3)
anova(model_tv, model_tv_poly3)$"Pr(>F)"[2]

```

As shown above, a low p-value for the anova test suggests that the additional polynomials added to the model are significant, so over the simple SLR, we perfer the 3rd order polynomial model.


```{r}
anova(model_tv_poly3, model_tv_poly5)
anova(model_tv_poly3, model_tv_poly5)$"Pr(>F)"[2]

```

As shown above, a low p-value for the anova test suggests that the additional polynomials added to the model are significant, so over the 3rd order polynomial, we perfer the 5th order polynomial model.

```{r}
anova(model_tv_poly5, model_tv_poly7)
anova(model_tv_poly5, model_tv_poly7)$"Pr(>F)"[2]

```

As shown above, a low p-value for the anova test suggests that the additional polynomials added to the model are not significant, so the 5th order polynomial is perferred over the 7th degree polynomial.

```{r}
#influential points in the model

infl_value = 4/length(cooks.distance(model_tv_poly5))

which(cooks.distance(model_tv_poly5) > infl_value)


```

So based on the `Fitted vs Residual` Plots, it appears that the model with the 5th degree polynomial is preferred.  The BPTest and the Shapiro-Wilk tests were not very helpful in selecting the model, but the anova test appeared to confirm the 5th degree polynomial model was the correct selection.

## Exercise 5 (Brains)

The data set `mammals` from the `MASS` package contains the average body weight in kilograms $(x)$ and the average brain weight in grams $(y)$ for $62$ species of land mammals. Use `?mammals` to learn more.

```{r, message = FALSE, warning = FALSE}
library(MASS)
```

**(a)** Plot average brain weight $(y)$ versus average body weight $(x)$.

```{r}
plot(brain ~ body, data = mammals,
     xlab = "Average body weight in kg",
     ylab = "Average brain weight in g",
     main = "Average Brain weight (g) vs Body Weight (kg) in 62 Species",
     pch  = 20,
     cex  = 2,
     col  = "dodgerblue")
```


**(b)** Fit a linear model with `brain` as the response and `body` as the predictor. Test for significance of regression. Do you think this is an appropriate model?

```{r}
model_brains = lm(brain ~ body, data = mammals)
#pvalue for significance of regression
summary(model_brains)$coefficients[2, 4]

#F test statistic for significance of regression
anova(model_brains)$"F value"[1]
#confirming same p-value 
(anova(model_brains))$"Pr(>F)"[1]
```

While the significance of regression test does indicate that the linear relationship is significant with the test statistic F = `r anova(model_brains)$"F value"[1]`, and a very low p-value `r (anova(model_brains))$"Pr(>F)"[1]`, **it does not** appear the this is the appropriate model looking at the fitted vs residuals, there appears to be a pattern and the large tails on the Q-Q plot suggest the normality is suspect.

The low p-value for the Shapiro-Wilk test suggests that the normality is suspect, and the bptest has a low p-value showing the equal variance is suspect.  These all suggest this is not the appropriate model.


```{r, fig.height = 5, fig.width = 10}
par(mfrow = c(1, 2))
plot(fitted(model_brains), resid(model_brains), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals",
     main = "Fitted vs Residuals")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)

qqnorm(resid(model_brains), main = "Normal Q-Q Plot, SLR brain ~ body", col = "darkgrey")
qqline(resid(model_brains), col = "dodgerblue", lwd = 2)
```

```{r}
shapiro.test(resid(model_brains))
```

```{r}
bptest(model_brains)
```


**(c)** Since the body weights do range over more than one order of magnitude and are strictly positive, we will use $\log(\text{body weight})$ as our *predictor*, with no further justification. (Recall, *the log rule*: if the values of a variable range over more than one order of magnitude and the variable is strictly positive, then replacing the variable by its logarithm is likely to be helpful.) Use the Box-Cox method to verify that $\log(\text{brain weight})$ is then a "recommended" transformation of the *response* variable. That is, verify that $\lambda = 0$ is among the "recommended" values of $\lambda$ when considering,

\[
g_\lambda(y) = \beta_0 + \beta_1 \log(\text{body weight})+\epsilon
\]

Include the relevant plot in your results, using an appropriate zoom onto the relevant values.

Below is the boxcox plot, that has a $\lambda$ value of 0 contained within the 95% confidence interval that includes 0 inside the interval.  This indicates that $\lambda$ = 0 is amoung the "recommended" values of $\lambda$

```{r, fig.height = 5, fig.width = 5}

model_boxcox = lm((brain) ~ log(body), data = mammals)
boxcox(model_boxcox, plotit = TRUE, lambda = seq(-0.05, 0.07, by = 0.01))

```

\[
g_\lambda(y) = \left\{
\begin{array}{lr}\displaystyle\frac{y^\lambda - 1}{\lambda} &  \lambda \neq 0\\
        & \\
       \log(y) &  \lambda = 0
     \end{array}
   \right.
\]
**(d)** Fit the model justified in part **(c)**. That is, fit a model with $\log(\text{brain weight})$ as the response and $\log(\text{body weight})$ as a predictor. Plot $\log(\text{brain weight})$ versus $\log(\text{body weight})$ and add the regression line to the plot. Does a linear relationship seem to be appropriate here?

```{r, fig.height = 5, fig.width = 10}
#fit the model
brains_body_loglog = lm(log(brain) ~ log(body), data = mammals)

par(mfrow = c(1, 2))

plot(log(brain) ~ log(body), data = mammals, col = "dodgerblue", pch = 20, cex = 1.5,
     main = "log(brain) vs log(body)")
abline(brains_body_loglog, col = "darkorange", lwd = 2)

plot(fitted(brains_body_loglog), resid(brains_body_loglog), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)
```

```{r}
shapiro.test(resid(brains_body_loglog))
```

```{r}
bptest(brains_body_loglog)
```

The plot of the `log(brain) vs log(body)` seems to fit the data well, and the `Fitted vs Residuals` plot seems to suggest linearity since for a given point it appears to be centered at 0.  The BP-Test has a high p-value indicating that this data has equal variance, and a high p-value for the Shapiro-Wilk tests suggest $\epsilon$ follows a normal distribution.

**(e)** Use a Q-Q plot to check the normality of the errors for the model fit in part **(d)**.


```{r}
qqnorm(resid(brains_body_loglog), col = "darkgrey")
qqline(resid(brains_body_loglog), col = "dodgerblue", lwd = 2)
```

**(f)** Use the model from part **(d)** to predict the brain weight of a male Snorlax which has a body weight of 1014.1 pounds. (A Snorlax would be a mammal, right?) Construct a 90% prediction interval.

```{r}
new_data = data.frame(body = 0.453592 * 1014.1)
predict(brains_body_loglog, newdata = new_data, interval = c("prediction"), level = 0.90)

exp(predict(brains_body_loglog, newdata = new_data, interval = c("prediction"), level = 0.90)[1,2])

exp(predict(brains_body_loglog, newdata = new_data, interval = c("prediction"), level = 0.90)[1,3])

```

The 90% prediction interval is interval is:
\[ (`r exp(predict(brains_body_loglog, newdata = new_data, interval = c("prediction"), level = 0.90)[1,2])`, `r exp(predict(brains_body_loglog, newdata = new_data, interval = c("prediction"), level = 0.90)[1,3])`) \]
