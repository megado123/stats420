---
title: "Facebook Dataset Analysis Final Project"
author: "Jianye Han, Zach Litz, Megan Masanz"
date: "July 25, 2017"
output:
  html_document: 
    toc: yes
  pdf_document: default
---

### Introduction

### Methods

#### Variable Exploration

#####1.  Variable Exploration Additive Model

Below is loading the packages used during the analysis
```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
library(MASS)
library(readr)
library(lmtest)
library(car)
library(knitr)
library(faraway)
```

Below is reading in the dataset as a table given there are spaces in the variable names.  In addition the variables that are required to be set as factors are set to factors here.
```{r}
fb = read.table("dataset_Facebook.csv", sep = ";", header = T)

fb$Type           = as.factor(fb$Type)
fb$Category       = as.factor(fb$Category)
fb$Post.Month     = as.factor(fb$Post.Month)
fb$Post.Hour      = as.factor(fb$Post.Hour)
fb$Post.Weekday   = as.factor(fb$Post.Weekday)
fb$Paid           = as.factor(fb$Paid)
```

Initial full additive model.
```{r}
full_model = lm(Lifetime.Post.Consumers ~ ., data = fb)
variables = colnames(fb)
```

The variables to analyze can be found below

####1.1 Table of predictor variables
```{r}
knitr::kable(colnames(fb), caption = "Variables for Analysis")
```

```{r}
#Perfect correlation so removed
fb$Total.Interactions = NULL
full_model = lm(Lifetime.Post.Consumers ~ ., data = fb)
```



```{r}
#number of rows
nrow(fb)
#Cleansing Data
fb = fb[complete.cases(fb), ]
nrow(fb)

#Initial Analysis:
variables = colnames(fb)
predictor_variables = variables[which(variables != "Lifetime.Post.Consumers")]
response_variable = "Lifetime.Post.Consumers"
```


First we will look at the $R{j}^2$ for each of the predictor variables.  Recall the $R{j}^2$ is the proportion of observed variation in the j-th predictor explained by the other predictors.

```{r message = FALSE, warning = FALSE}
get_RjSquared = function(predictor_variables){
  rj_squared = rep(0, length(predictor_variables))
  for (i in 1: length(predictor_variables))
  {
    strModel     = paste(predictor_variables[i], "~ . - Lifetime.Post.Consumers")
    temp_model   = lm(strModel, data = fb)
    rj_squared[i] = summary(temp_model)$r.squared
  }
  data.frame(predictor_variables = predictor_variables, rj_squared = rj_squared )
}

RjsquaredForPredictors = get_RjSquared(predictor_variables)



```

####1.1.2 Table of predictor variables
```{r}
knitr::kable(RjsquaredForPredictors, caption = "Rj^2")
```

As shown in table [1.1.2 Table of predictor variables], it appears that the variation in our predictors can be explained by other variables suggestig that we should pair down which predictors we include in the model. Also note that the categorical predictors come in as `NA` which does make sense given for a categorical variable, it would be easy to provide a $R{j}^2$ value after breaking them down into several variables.
 
This information suggests that we will have high VIF values for our predictors. 


```{r}
vif(full_model)
```

Using the variance inflaction factor to quantify the impact of collinearity on the variance of the regression estimates, we know that this will be a bad model for explaining the relationship between the response and predictors requiring the model to be simplied

Given the large number of predictors, we will begin with attempting to understand which predictor variables should be explored.  The function `getAdditiveModels` will git a model starting with 1 $\beta$ parameter for each of the parameters that were found in [1.1 Table of predictor variables].  For a given $\beta$ parameter, the model fit with the lowest RSS will then be used to fit the next size of $\beta$ parameters.  

The function `getModels` will retrieve for every number of $\beta$ parameters the model with the lowest RSS.

the function `doAnova` finally will do an Anova F-Test to determine which is the preferred model, comparing model with $\beta$ parmaters of size `i` to that of `i+1`.  Once the Anova F-Test values **fail** to produce a better model, the function stops determining the best MLR additive model has been determined.

```{r}
getAdditiveModels = function(variables, response_variable, data_set, alpha)
{
  num_predictor_variables = length(variables)
  p_val           = rep(0, num_predictor_variables ^ 2)
  bp_p_value      = rep(0, num_predictor_variables ^ 2)
  shapiro_p_value = rep(0, num_predictor_variables ^ 2)
  strmodel        = rep(0, num_predictor_variables ^ 2)
  rss             = rep(0, num_predictor_variables ^ 2)
  beta_parameter  = rep(0, num_predictor_variables ^ 2)
  pass_bp         = rep(0, num_predictor_variables ^ 2)
  pass_shapiro    = rep(0, num_predictor_variables ^ 2)
  adjustedR2      = rep(0, num_predictor_variables ^ 2)
  sizeResiduals   = rep(0, num_predictor_variables ^ 2)
  
  index = 1;
  
  for (j in 1:num_predictor_variables)
  {
    smallestrss = 0
    smallestrssIndex = 0 
    for (i in 1:length(variables))
    {
      if(j == 1) #at the beginning
      {
        startpointer          = index
        #could make s1 be log(y)
        s1                    = response_variable
        s2                    = variables[i]
        strmodel[index]       = paste(s1, " ~ " , s2)
      }
      else
      {
        s2                    = variables[i]
        strmodel[index]       = paste(previousBestModel , " + " , s2)
      }
      
      model                    = lm(strmodel[index], data = data_set)
      rss[index]               = sum(resid(model)^2) 
      bp_p_value[index]        = bptest(model)$p.value
      shapiro_p_value[index]   = shapiro.test(resid(model))$p.value
      beta_parameter[index]    = j
      adjustedR2[index]        = summary(model)$adj.r.squared
      sizeResiduals[index]     = length(resid(model))
      
      if (shapiro_p_value[i] > alpha) {
        pass_shapiro[index] = TRUE
      }
      
      if (bp_p_value[i] > alpha) {
        pass_bp[index] = TRUE
      }
      
      if (smallestrssIndex == 0 || rss[index] < smallestrss){
        smallestrssIndex = index
        smallestrss = rss[index]
      }
      index = index + 1
    }
    #for a given number of predictor variables create temp vector and find the min rss
    previousBestModel = strmodel[smallestrssIndex]
  }
  result         = data.frame(beta_parameter, strmodel, rss, adjustedR2, 
                              bp_p_value, shapiro_p_value, pass_bp, pass_shapiro, sizeResiduals)
  result$strmodel =  as.character(result$strmodel)
  result
}



#here is a function that for a given number of beta parameters pulls out the lowest rss
getModels = function(results, num_predictor_variables){
  bestModelForParamNumber = rep("", num_predictor_variables)
  for (i in 1:num_predictor_variables)
  {
    subset_data = results[results$beta_parameter == i, ]
    bestModelForParamNumber[i] = subset_data$strmodel[which.min(subset_data$rss)]
  }
  bestModelForParamNumber
}



#now for each of the "best" linear models, do an anova test to determine which are actually the best
doAnova = function(dataset, models, alpha)
{
  p_val         = rep(0, length(models) - 1)
  keep_old      = rep(FALSE, length(models) - 1)
  winning_model = rep(0, length(models) - 1)
  stop = FALSE
  for (i in 1:(length(models) -1))
  {
    if (stop == FALSE)
    {
      p_val[i] = anova(lm(bestModels[i], data = dataset), lm(bestModels[i + 1], data = dataset))$"Pr(>F)"[2]
      if (p_val[i] < alpha)
      {
        keep_old[i]      = FALSE
        winning_model[i] = bestModels[i + 1]
        stop             = FALSE
      }
      else
      {
        keep_old[i]      = TRUE
        winning_model[i] = bestModels[i]
        report_winning   = winning_model[1:i]
        report_p_val     = p_val[1:i]
        report_keep_old  = keep_old[1:i]
        stop             = TRUE
        i                = length(models) -1
      }
    }
  }
  
  output                = data.frame(report_winning, report_p_val, report_keep_old)
  output$report_winning =  as.character(output$report_winning)
  output
}

results = getAdditiveModels(predictor_variables, response_variable, fb, .05)
bestModels = getModels(results, length(predictor_variables))
results = doAnova(fb, bestModels, 0.05 )
model_data_selected_RSS_Manual = results[results$report_keep_old == TRUE, ]
model_selected_RSS_Manual      = lm(model_data_selected_RSS_Manual$report_winning, data = fb)

#Model from Manual Selection
model_data_selected_RSS_Manual$report_winning

#best additive model based on RSS and Anova Test
#Lifetime.Post.Consumers  ~  Lifetime.Engaged.Users  +  like  +  share  +  Lifetime.People.who.have.liked.your.Page.and.engaged.with.your.post  +  Post.Month  +  Lifetime.Post.Consumptions  +  comment  +  Lifetime.Post.Total.Reach
```

```{r}
vif(model_selected_RSS_Manual)
```

The variance inflation factors for this model are much improved from the model including all predictors, however we still see values greater than 5.  Given we are looking for a model to explain, we will continue with evaluation of model selection using backwards and forwards BIC and AIC to determine if a simplier additive model can be achieved before attempting to find a simple model for explaining the response variable.

At this point, we will remove the cateogical variables from the analysis and evaluate adding them back in after simplifying the model down to reduce complexity of our evaluation.  Using R to create models using AIC and BIC (forwards and backwards) R will automatically create dummy variables for us, but that will introduce many variables and at this point in the analylsis we will prevent that by excluding these variables from the analysis at this stage.  Note the the manual RSS selection process did indicate the `Post.Month` was significant when we assumed an $\alpha$ = 0.05.

Due to this descision, we will again run the code above but excluding the categorical variable to assist in model comparision process.

```{r}
variables

predictor_variables = variables[which(variables != "Lifetime.Post.Consumers" 
                                      & variables != "Type" & variables != "Category" & variables != "Post.Month"
                                      & variables != "Post.Weekday" 
                                      & variables != "Post.Hour" & variables != "Paid" )]

#Variables for analysis
predictor_variables


results = getAdditiveModels(predictor_variables, response_variable, fb, .05)
bestModels = getModels(results, length(predictor_variables))
results = doAnova(fb, bestModels, 0.05 )
model_data_selected_RSS_Manual = results[results$report_keep_old == TRUE, ]
model_selected_RSS_Manual      = lm(model_data_selected_RSS_Manual$report_winning, data = fb)

#Model from Manual Selection
model_data_selected_RSS_Manual$report_winning
```




The function `createStringFullModel` is simply used to create model given the large number of predictor variables.  The string will be used in the AIC methods.

```{r}
###########################################################
#AIC and BIC
##########################################################



createStringFullModel = function(predictor_variables){
  num_predictor_variables = length(predictor_variables)
  for (i in 1:num_predictor_variables)
  {
    if (i == 1)
    {
      model = paste("Lifetime.Post.Consumers", "~" ,predictor_variables[i] )
    }
    else
      model = paste(model , " + " , predictor_variables[i])
  }
  model
}

strFullModel = createStringFullModel(predictor_variables)


##########################################################
#AIC - less aggressive
##########################################################
#forward AIC
fb_mod_start = lm(Lifetime.Post.Consumers ~ 1, data = fb)
model_forward_aic = step(fb_mod_start,
                         strFullModel,
                         direction = "forward", trace = 0)
```



```{r}
summary(model_forward_aic)
```


```{r}

vif(model_forward_aic)

vif(model_selected_RSS_Manual)
```

These 2 methods appear to select very similiar models, with the exception of forward AIC chosing an additional parameter `Lifetime.Post.Impressions.by.people.who.have.liked.your.Page` which does appear to have increased the VIF significantly, so we would probably choose to reject adding this variable to our model.  


####1.1.2 Variables Added Plot comparing RSS manually selected model with the addition suggested by forward AIC
```{r}
#variable added plot
ImpressionsModel = lm(Lifetime.Post.Impressions.by.people.who.have.liked.your.Page ~ 
                        Lifetime.Engaged.Users + like + share + 
                        Lifetime.People.who.have.liked.your.Page.and.engaged.with.your.post + 
                        Lifetime.Post.Consumptions + 
                        comment + Page.total.likes + 
                        Lifetime.Post.Total.Reach + 
                        Lifetime.Post.Total.Impressions, data  = fb)

plot(resid(model_selected_RSS_Manual) ~ resid(ImpressionsModel), col = "dodgerblue", pch = 20,
     xlab = "Residuals, Added Predictor", ylab = "Residuals, Original Model")
abline(h = 0, lty = 2)
abline(v = 0, lty = 2)
abline(lm(resid(model_selected_RSS_Manual) ~ resid(ImpressionsModel)),
       col = "darkorange", lwd = 2)
```

The partial correlation coefficient of `Lifetime.Post.Impressions.by.people.who.have.liked.your.Page` and `Lifetime.Post.Consumers` with the effects of the other variables in the model removed serves as additional justication that this predictors should not end up in our final selected model.
```{r}
cor(resid(ImpressionsModel), resid(model_selected_RSS_Manual))
```



Next we examine backwards AIC
```{r}
fullModel = lm(strFullModel, data = fb)
model_back_aic = step(fullModel, direction = "backward", trace = 0)
summary(model_back_aic)
vif(model_back_aic)
vif(model_forward_aic)
```

The models look very similiar - we will next investigate a more aggressive parameter selecting method with a higher penality for a higher number of beta parameters - BIC.

```{r}
##########################################################
#BIC - more aggressive
##########################################################
full_model = lm(Lifetime.Post.Consumers ~ ., data = fb)
n = length(resid(full_model))
model_forward_bic = step(fb_mod_start,
                         strFullModel,
                         direction = "forward", k = log(n), trace = 0)
summary(model_forward_bic)
```


```{r}
vif(model_forward_bic)

#backward BIC
model_back_bic = step(full_model, direction = "backward", k = log(n), trace = 0)
summary(model_back_bic)

###########################################################
#Make a table saying which variables are in which model
#to specify which variables to explore
##########################################################
                                                         
all = predictor_variables %in% names(coef(model_selected_RSS_Manual)) %in%  names(coef(model_forward_aic)) %in% names(coef(model_back_aic)) %in% names(coef(model_forward_bic)) 
           
Variables_RSS_Manual   = predictor_variables %in% names(coef(model_selected_RSS_Manual))

Variables_Forward_AIC  = predictor_variables %in% names(coef(model_forward_aic))

Variables_Backward_AIC = predictor_variables %in% names(coef(model_back_aic))

Variables_Forward_BIC  = predictor_variables %in% names(coef(model_forward_bic))


Variables_Backward_BIC = predictor_variables %in% names(coef(model_back_bic))

#Variable, RSS Evelaution per beta parameter, Backward BIC, Forward BIC, Backward AIC, Forward AIC
variables_for_analysis = data.frame(predictor_variables, Variables_RSS_Manual, Variables_Forward_BIC, Variables_Backward_BIC, Variables_Forward_AIC, Variables_Backward_AIC)
```

####1.2 Table of predictor variables selected for additive MLR model based on different methods
```{r}
colnames(variables_for_analysis) = c("Variables" , "Variable included in RSS Manual", "Variable included in Forward BIC", "Variable included in Backward BIC", "Variable included in Forward AIC", "Variable included in Backward AIC")
knitr::kable(variables_for_analysis, caption = "MLR Additive Model Variable Analysis")
```

This table [####1.2 Table of predictor variables selected for additive MLR model based on different methods] gives us a good idea of what variables we should use in our analysis.  We can now begin with looking at a pairs plot with the subset of variables that are included in all of the additive models to better understand the predictor variables themselves.


```{r}
fb_subset = subset(fb, select = c("Lifetime.Post.Consumers", "Lifetime.Post.Total.Reach", "Lifetime.Engaged.Users", "Lifetime.Post.Consumptions", "Lifetime.People.who.have.liked.your.Page.and.engaged.with.your.post", "comment", "like", "share"))

```

The pairs plot for this subset of variables is found below
####1.2 Pairs plot of subset of predictor variables for analysis
```{r fig.height=20, fig.width=10}
pairs(fb_subset, col = "dodgerblue")
```

With the pairs plot, we can see it appears some relationships could be looked at, but quickly seeing the order of magnitude on the reponse variable, suggests we should also do a box-cox plot to determine if perhaps the response should be transformed.

####1.3 Box-Cox plots for MLR Models
```{r}
boxcox(model_selected_RSS_Manual, plotit = TRUE)

par(mfrow = c(2,2))
boxcox(model_forward_aic, plotit = TRUE)
boxcox(model_back_aic, plotit = TRUE)
boxcox(model_forward_bic, plotit = TRUE)
boxcox(model_back_bic, plotit = TRUE)

```

The box-cox plot for all the models currently created suggests a transformation of the response variable.  We will add a new response variable to the dataset to enable the Box-Cox plots to reflect this relationship

####1.4 Box-Cox plots for MLR Models with log Response added
```{r fig.height=20, fig.width=10}
fb_subset = subset(fb, select = c("Lifetime.Post.Consumers", "Lifetime.Post.Total.Reach", "Lifetime.Engaged.Users", "Lifetime.Post.Consumptions", "Lifetime.People.who.have.liked.your.Page.and.engaged.with.your.post", "comment", "like", "share"))

log.Consumers = log(fb_subset$Lifetime.Post.Consumers)

fb_withResponseTransform = cbind( fb_subset, log.Consumers)
pairs(fb_withResponseTransform, col = "dodgerblue")

```

The pairs plot shown above in [1.4 Box-Cox plots for MLR Models with log Response added] makes seeing a few trends much easier with the addition of the log.Consumers.  Several variables appear to either require a log transformation, or perhaps a power transforation.  Given we currently have 7 predictors of interest, a function was written to automate reviewing this information.

Generically we will add + 1 to each predictor variable for log transformation to ensure the values are greater than 0 if the min val

```{r}
min(fb_withResponseTransform$Lifetime.Post.Total.Reach)
min(fb_withResponseTransform$Lifetime.Engaged.Users)
min(fb_withResponseTransform$Lifetime.Post.Consumptions)
min(fb_withResponseTransform$Lifetime.People.who.have.liked.your.Page.and.engaged.with.your.post)
min(fb_withResponseTransform$comment)
min(fb_withResponseTransform$like)
min(fb_withResponseTransform$share)

```



The function below `transformationsPowers` automates checking for transformation.  We are looking for a model for **explanation** so ensuring model assumptions are met is important in ensuring our model can explain well.
```{r}

transformationsPowers = function(predictor, response, dataset, responsevector, predictorvector){
  
  strQuad                      = paste("log(", response, ") ~ ", predictor , " + I(" , predictor, "^ 2) ")
  quad_model                   = lm(strQuad, data = dataset)
  quad_RMSE                    = sqrt(mean((responsevector - exp(fitted(quad_model))) ^ 2))
  quad_bp_p_value              = bptest(quad_model)$p.value
  quad_shapiro_p_value         = shapiro.test(resid(quad_model))$p.value
  
  strThird                     = paste("log(", response, ") ~ ", predictor , " + I(" , predictor, "^ 2)  + I(", predictor, "^3)")
  third_model                  = lm(strThird, data = dataset)
  third_RMSE                   = sqrt(mean((responsevector - exp(fitted(third_model))) ^ 2))
  third_bp_p_value             = bptest(third_model)$p.value
  third_shapiro_p_value        = shapiro.test(resid(third_model))$p.value
 
  predictor   = rep(predictor, 2)
  transform   = c("^2 obeying hierachy", "^3 obeying hierachy")
  bp          = c(quad_bp_p_value, third_bp_p_value)
  shapiro     = c(quad_shapiro_p_value, third_shapiro_p_value)
  RMSE        = c(quad_RMSE, third_RMSE)
  
  data.frame(predictor = predictor, transform = transform, bp = bp, shapiro = shapiro, RMSE = RMSE)
  
}

variables = colnames(fb_withResponseTransform)
predictor_variables = variables[which((variables != "Lifetime.Post.Consumers") & variables != "log.Consumers")]

length(predictor_variables)
```

####1.5 Transformation Analysis
```{r}
result1 = transformationsPowers(predictor_variables[1], "Lifetime.Post.Consumers", fb_withResponseTransform, fb_withResponseTransform$Lifetime.Post.Consumers)
knitr::kable(result1, caption = "Transformation Review")

result1 = transformationsPowers(predictor_variables[2], "Lifetime.Post.Consumers", fb_withResponseTransform, fb_withResponseTransform$Lifetime.Post.Consumers)
knitr::kable(result1, caption = "Transformation Review")

result1 = transformationsPowers(predictor_variables[3], "Lifetime.Post.Consumers", fb_withResponseTransform, fb_withResponseTransform$Lifetime.Post.Consumers)
knitr::kable(result1, caption = "Transformation Review")

result1 = transformationsPowers(predictor_variables[4], "Lifetime.Post.Consumers", fb_withResponseTransform, fb_withResponseTransform$Lifetime.Post.Consumers)
knitr::kable(result1, caption = "Transformation Review")

result1 = transformationsPowers(predictor_variables[5], "Lifetime.Post.Consumers", fb_withResponseTransform, fb_withResponseTransform$Lifetime.Post.Consumers)
knitr::kable(result1, caption = "Transformation Review")

result1 = transformationsPowers(predictor_variables[6], "Lifetime.Post.Consumers", fb_withResponseTransform, fb_withResponseTransform$Lifetime.Post.Consumers)
knitr::kable(result1, caption = "Transformation Review")

result1 = transformationsPowers(predictor_variables[7], "Lifetime.Post.Consumers", fb_withResponseTransform, fb_withResponseTransform$Lifetime.Post.Consumers)
knitr::kable(result1, caption = "Transformation Review")
```

The results 


```{r}
transformationsLog = function (response, predictors, dataset, responsevector){
  log_RMSE             = rep(0, length(predictors) )
  log_bp_p_value       = rep(0, length(predictors))
  log_shapiro_p_value  = rep(0, length(predictors))
  log_model_adjust_r2  = rep(0, length(predictors))
  
  quad_RMSE            = rep(0, length(predictors) )
  quad_bp_p_value      = rep(0, length(predictors) )
  quad_shapiro_p_value = rep(0, length(predictors) )
  quad_model_adjust_r2 = rep(0, length(predictors))
  
  third_RMSE            = rep(0, length(predictors) )
  third_bp_p_value      = rep(0, length(predictors) )
  third_shapiro_p_value = rep(0, length(predictors) )
  third_model_adjust_r2 = rep(0, length(predictors))
  
  log_strModel             = rep(0, length(predictors) )
  quad_strModel          = rep(0, length(predictors) )
  third_strModel         = rep(0, length(predictors) )

  
for (i in 1: length(predictors)){

  strLog = paste("log(", response, ") ~ log(", predictors[i] , ")")

  log_model = lm(strLog, data = dataset)
  
  log_RMSE[i]                     = sqrt(mean((responsevector - exp(fitted(log_model))) ^ 2))
  log_bp_p_value[i]               = bptest(log_model)$p.value
  log_shapiro_p_value[i]          = shapiro.test(resid(log_model))$p.value
  log_model_adjust_r2[i]          = summary(log_model)$adj.r.squared
  log_strModel[i]                 = strLog

  
    strQuad                      = paste("log(", response, ") ~ ", predictors[i] , " + I(" , predictors[i], "^ 2) ")
  
  quad_model                   = lm(strQuad, data = dataset)
  
  quad_RMSE[i]                    = sqrt(mean((responsevector - exp(fitted(quad_model))) ^ 2))
  quad_bp_p_value[i]              = bptest(quad_model)$p.value
  quad_shapiro_p_value[i]         = shapiro.test(resid(quad_model))$p.value
  quad_model_adjust_r2[i]          = summary(quad_model)$adj.r.squared
  quad_strModel[i]                 = strQuad
  
  strThird                     = paste("log(", response, ") ~ ", predictors[i] , " + I(" , predictors[i], "^ 2)  + I(", predictors[i], "^3)")
  third_model                  = lm(strThird, data = dataset)
  
  third_RMSE[i]                   = sqrt(mean((responsevector - exp(fitted(third_model))) ^ 2))
  third_bp_p_value[i]             = bptest(third_model)$p.value
  third_shapiro_p_value[i]        = shapiro.test(resid(third_model))$p.value
  third_model_adjust_r2[i]          = summary(third_model)$adj.r.squared
  third_strModel[i]                 = strThird
  
  transform   = c(rep("log", 7), rep("^2 with hiearchy", 7), rep("^3 with hierarchy", 7))
  strModel    = c(log_strModel, quad_strModel, third_strModel)
  bp          = c(log_bp_p_value,      quad_bp_p_value,      third_bp_p_value)
  shapiro     = c(log_shapiro_p_value, quad_shapiro_p_value, third_shapiro_p_value)
  RMSE        = c(log_RMSE,            quad_RMSE,            third_RMSE)
  adj_r_squard = c(log_model_adjust_r2, quad_model_adjust_r2, third_model_adjust_r2)
  
}
 
  data.frame( model = strModel, predictor = rep(predictors, 3), bp = bp, shapiro = shapiro, RMSE = RMSE, adj_r_squard = adj_r_squard )
  
}
```

In order to explore log transformations on predictor variables, in the case when the range is from 0 to some value, we will add 1 to the predictor variables given their min value is at 0. 

```{r}
fb_withResponseTransformForLog = fb_withResponseTransform
fb_withResponseTransformForLog$like = fb_withResponseTransformForLog$like + 1
fb_withResponseTransformForLog$share = fb_withResponseTransformForLog$share + 1
fb_withResponseTransformForLog$comment = fb_withResponseTransformForLog$comment + 1

(predictor_variables)
length(predictor_variables)

result1 = transformationsLog("Lifetime.Post.Consumers", predictor_variables, fb_withResponseTransformForLog, fb_withResponseTransformForLog$Lifetime.Post.Consumers )

knitr::kable(result1, caption = "Transformation Review with transformed predictors added 1")
```


```{r}
##function
diagnostics = function(model, pcol= 2, lcol = 1, alpha = 0.05, plotit = TRUE, testit = TRUE){
  if (plotit == TRUE){
    par(mfrow = c(1, 2))
    plot(fitted(model), resid(model), col = pcol, pch = 20,
         xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals")
    abline(h = 0, col = lcol, lwd = 2)
    qqnorm(resid(model), main = "Normal Q-Q Plot", col = pcol)
    qqline(resid(model), col = lcol, lwd = 2)
  }
  if (testit == TRUE){
    shapiro_p_value = shapiro.test(resid(model))$p.value
    bp_p_value      = bptest(model)$p.value 
    bp_decision      = ifelse(bp_p_value < alpha, "Reject Equal Variance - Bad", "Fail to Equal Variance-Good")
    shapiro_decision = ifelse(shapiro_p_value < alpha, "Reject Normality - Bad", "Fail to Reject Normality- Good")
    
    return_val = list(shapiro_p_value = shapiro_p_value, bp_p_value = bp_p_value, bp_decision = bp_decision, shapiro_decision = shapiro_decision)
    return_val
  }
}
```


```{r message = FALSE, warning = FALSE}
#dataset to use for analysis
m1 = lm(log(Lifetime.Post.Consumers) ~ log(like), data = fb_withResponseTransformForLog, 
        testit = TRUE, pcol = "darkorange", lcol = "dodgerblue", plotit = TRUE)

m2 = lm(log(Lifetime.Post.Consumers) ~ log(like) + like, data = fb_withResponseTransformForLog, 
        testit = TRUE, pcol = "darkorange", lcol = "dodgerblue", plotit = TRUE)

m3 = lm(log(Lifetime.Post.Consumers) ~ log(share), data = fb_withResponseTransformForLog, 
        testit = TRUE, pcol = "darkorange", lcol = "dodgerblue", plotit = TRUE)

m4 = lm(log(Lifetime.Post.Consumers) ~ log(share) + share, data = fb_withResponseTransformForLog, 
        testit = TRUE, pcol = "darkorange", lcol = "dodgerblue", plotit = TRUE)

m5 =  lm(log(Lifetime.Post.Consumers) ~ like + log(like) + log(share) + share, data = fb_withResponseTransformForLog,
                  testit = TRUE, pcol = "darkorange", lcol = "dodgerblue", plotit = TRUE)

diagnostics(m1, testit = TRUE, pcol = "darkorange", lcol = "dodgerblue", plotit = TRUE)
diagnostics(m2, testit = TRUE, pcol = "darkorange", lcol = "dodgerblue", plotit = TRUE)
diagnostics(m3, testit = TRUE, pcol = "darkorange", lcol = "dodgerblue", plotit = TRUE)
diagnostics(m4, testit = TRUE, pcol = "darkorange", lcol = "dodgerblue", plotit = TRUE)
diagnostics(m5, testit = TRUE, pcol = "darkorange", lcol = "dodgerblue", plotit = TRUE)
```

#### Factor Variables

To further investigate creation of a model for explanation, recall that the RSS Manual model included the variable `Post.Month` so we will further investigate if putting this variable into the model will be helpful.

```{r}
fb_subset = subset(fb, select = c("Lifetime.Post.Consumers", "Lifetime.Post.Total.Reach", "Lifetime.Engaged.Users", "Lifetime.Post.Consumptions", "Lifetime.People.who.have.liked.your.Page.and.engaged.with.your.post", "comment", "like", "share", "Post.Month", "Post.Weekday", "Post.Hour", "Paid"))

fb_subset$like = fb_subset$like + 1
fb_subset$share = fb_subset$share + 1
fb_subset$comment = fb_subset$comment + 1


current_best_model = lm(log(Lifetime.Post.Consumers) ~ like + log(like) + log(share) + share, data = fb_subset)

model_month = lm(log(Lifetime.Post.Consumers) ~ like + log(like) + log(share) + share + Post.Month, data = fb_subset)

summary(model_month)

anova(current_best_model, model_month)

```
According to the anova test, this addition is significant.  Let's examine the assumptions of the model to determine if they are still met.  Both the BP test and Shapiro-Wilk test fail but adding these into the model.  Perhaps 

```{r}
diagnostics(model_month, testit = TRUE, pcol = "darkorange", lcol = "dodgerblue", plotit = TRUE)
```

```{r}

model_int_forward_aic = step(current_best_model,
                         log(Lifetime.Post.Consumers) ~ like + log(like) + log(share) + share + Post.Month,
                         direction = "forward", trace = 0)

summary(model_int_forward_aic)
```

The variables that seem to be the most significant are `Post.Month12` and `Post.Month11`.  We will explore creating dummy variables to look specifically at these 2 months to determine if they could be included in the model.

```{r}
PostMonth12 = 1 * as.numeric(fb_subset$Post.Month == "12")
PostMonth11 = 1 * as.numeric(fb_subset$Post.Month == "11")

length(PostMonth12)
length(PostMonth11 )
nrow(fb_subset)

fb_data = cbind( fb_subset, PostMonth11, PostMonth12)

head(fb_data)
#nrow(fb_data)
#colnames(fb_data)


model_month11 = lm(log(Lifetime.Post.Consumers) ~ like + log(like) + log(share) + share 
                 + PostMonth11, data = fb_data)

current_best_model = lm(log(Lifetime.Post.Consumers) ~ like + log(like) + log(share) + share, data = fb_subset)
anova(current_best_model, model_month11)

diagnostics(model_month11, testit = TRUE, pcol = "darkorange", lcol = "dodgerblue", plotit = TRUE)

vif(model_month11)
```

Including when the `Post.Month` == 11 to our model as a dummy variable has improved our ability to fufill the assumptions of the model.  According to the anova test, this is significant.  Our VIF has increased between the variables, so we will need further exploration to determine which is the best model.

 

```{r}
model_month12 = lm(log(Lifetime.Post.Consumers) ~ like + log(like) + log(share) + share 
                 + PostMonth12, data = fb_data)

anova(current_best_model, model_month12)

diagnostics(model_month12, testit = TRUE, pcol = "darkorange", lcol = "dodgerblue", plotit = TRUE)


```

As demonstrated above, including `Post.Month` == 12 to our model did not improve our ability to meeting the assumptions of the model, so while it is significant, we would like to consider models that will do a good job of explaining.

Next we will look at including iteraction terms with this variable to determine if that will also improve our model. 

```{r}
fb_data$PostMonth11 = as.factor(fb_data$PostMonth11)

model_month11 = lm(log(Lifetime.Post.Consumers) ~ (like + log(like) + log(share) + share)
                 + PostMonth11, data = fb_data) 

model_month11_int = lm(log(Lifetime.Post.Consumers) ~ (like + log(like) + log(share) + share)
                 * PostMonth11, data = fb_data)

summary(model_month11_int)

anova(model_month11, model_month11_int)
```

Due to the anova F-Test, we will keep the initial model without interaction terms.

Currently we have 2 models that we will like to have further analysis on:

log(Lifetime.Post.Consumers) ~ like + log(like) + log(share) + share 
                 + PostMonth12

log(Lifetime.Post.Consumers) ~ like + log(like) + log(share) + share 
                 + PostMonth12


#### Influence, Leverages & Outliers

Next we will identify points of influence.

```{r}
model_month11 = lm(log(Lifetime.Post.Consumers) ~ (like + share + log(like) + log(share) )
                 + PostMonth11, data = fb_data) 

which.max(hatvalues(model_month11))

fb_data[which.max(hatvalues(model_month11)),]

#how many points have a large leverage
sum(hatvalues(model_month11) > 2 * mean(hatvalues(model_month11)))

rstandard(model_month11)[abs(rstandard(model_month11)) > 2]

```

**Need to examine what the model looks like especially the Fitted vs Residula plots with the outliers taken out. Does that improve our model?**

#### Validation

RMSE -
Confidence Intervals - 
LOOCV - 
Adjusted R^2 - 

```{r}
set.seed(42)
train_index = sample(1:nrow(fb_data), 250)
train = fb_data[train_index ,]
test  = fb_data[-train_index ,]

nrow(train)
nrow(test)
head(train$leverages)


RMSE= function(model, name, predictors) {

  RMSE_train   = sqrt( mean(  resid(model) ^ 2  ))
  RMSE_test  = sqrt(mean( (test$Lifetime.Post.Consumers - exp(predict(model, newdata = test)))^2 ))
  
  RMSE_train2 = sqrt(mean((train$Lifetime.Post.Consumers - exp(fitted(model))) ^ 2))
  

  

  values = c(
    Name       = name,
    RMSE_train = RMSE_train,
    RMSE_train2 = RMSE_train2,
    RMSE_test  = RMSE_test)
}

model_1 = lm(log(Lifetime.Post.Consumers) ~ like + log(like) + log(share) + share , data = train)
model_2 = lm(log(Lifetime.Post.Consumers) ~ like + log(like), data = train)
model_3 = lm(log(Lifetime.Post.Consumers) ~ log(share) + share, data = train)
model_4 = lm(log(Lifetime.Post.Consumers) ~ log(like) + log(share), data = train)



result = rbind( RMSE(model_1, "Model 1", "tax"),
                RMSE(model_2, "Model 2", "zn + nox + rm + dis + rad + tax + ptratio + black + lstat"),
                RMSE(model_3, "Model 3", "zn + nox + rm  + dis  + tax + ptratio + black + lstat"), 
                RMSE(model_4, "Model 4", "zn + nox + rm  + dis + black  + ptratio + lstat")
                )


kable(result, format = "markdown", padding = 3)
```


```{r}
calc_diags = function(model) {
  loocv_rmse        = sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
  adjusted_r2       = summary(model)$adj.r.squared
  bptest            = bptest(model)$p.value
  shapiro_p_value   = shapiro.test(resid(model))$p.value
  
  data.frame(loocv_rmse = loocv_rmse, adjusted_r2 = adjusted_r2, bptest = bptest, shapiro_p_value = shapiro_p_value)
}


model_1 = lm(log(Lifetime.Post.Consumers) ~ like + share + log(like) + log(share) + PostMonth11 , data = fb_data)
model_2 = lm(log(Lifetime.Post.Consumers) ~ like + log(like), data = fb_data)
model_3 = lm(log(Lifetime.Post.Consumers) ~ log(share) + share, data = fb_data)
model_4 = lm(log(Lifetime.Post.Consumers) ~ log(like) + log(share), data = fb_data)

calc_diags(model_1)
calc_diags(model_2)
calc_diags(model_3)
calc_diags(model_4)
```

Above we can see that the `best` model we came up with was `log(Lifetime.Post.Consumers) ~ like + share + log(like) + log(share) + PostMonth11`
### Results

### Discussion

### Appendix
