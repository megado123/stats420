---
title: "Week 6 - Simulation Study"
author: "STAT 420, Summer 2017, Megan Masanz, netid: mjneuman"
date: ''
output:
  html_document: 
    toc: yes
  pdf_document: default
---


## Simulation Study 1

###Introduction
This is a simulation study for a multiple linear regression using the model:

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3}  + \beta_4 x_{i4} + \epsilon_i
\]


where $\epsilon_i \sim N(0, \sigma^2)$ and 

- $\beta_0$ = 2,
- $\beta_1$ = 1,
- $\beta_2$ = 1,
- $\beta_3$ = 1,
- $\beta_4$ = 1.


During this simulation study we will discuss the distribution associated with $\hat{\beta}_1$, $s_e^2$, and $\hat{\text{E}}[Y \mid x_1 = -3, x_2 = 2.5, x_3 = 0.5, x_4 = 0]$

For this simulation study we will use the data stored in [`study_1.csv](study_1.csv) which contains values of the 4 predictors with a sample size of 15.  The code provided within the **methods** section will  run 3000 simulations for three different values of $\sigma \in (1, 5, 10)$.  The variables in the dataset are:

- `x1`
- `x2`
- `x3`
- `x4`

We know the true distributions of the estimates, so questions to be addressed during this simulation study are:

- What are the true distributions of the estimates for:
    + $\hat{\beta}_1$
    + $s_e^2$
    + $\hat{\text{E}}[Y \mid x_1 = -3, x_2 = 2.5, x_3 = 0.5, x_4 = 0]$
    
-How do the empirical distributions from the simulations compare to the true distributions?

-How do the distributions change when $sigma$ is changed

###Methods

```{r}

birthday = 19810803
set.seed(birthday)

library(readr)
study_1 = read_csv("study_1.csv")

num_sims = 3000 # number of simulations
n = 15          # sample size
p = 5           # number of beta parameters

beta_0 = 2
beta_1 = 1
beta_2 = 1
beta_3 = 1
beta_4 = 1
sigma_1 = 1
sigma_2 = 5
sigma_3 = 10

x0 = rep(1, n)
x1 = study_1$x1
x2 = study_1$x2
x3 = study_1$x3
x4 = study_1$x4
X = cbind(x0, x1, x2, x3, x4)
C = solve(t(X) %*% X)
y = rep(0, n)

xo = c(1, -3, 2.5, 0.5, 0)
sim_data = data.frame(x1, x2, x3, x4, y)

simulate = function(num_sims, sigma){
  
  beta_hat_1   = rep(0, num_sims)
  se_squared   = rep(0, num_sims) 
  y_hat_xo     = rep(0, num_sims)

  for(i in 1:num_sims) {
    #create noise
    eps = rnorm(n, mean = 0 , sd = sigma)
    #simulate y values
    sim_data$y = beta_0 * x0 + beta_1 * x1 + beta_2 * x2 + beta_3 * x3 + beta_4 * x4 + eps
    #fit the model
    fit = lm(y ~ x1 + x2 + x3 + x4, data = sim_data)
    #exttract values of interest
    beta_hat_1[i]   = coef(fit)[2]
    se_squared[i]   = (summary(fit)$sigma) ^ 2
    #calculate beta_hat for matrix multiplication to determine y_hat_xo
    beta_hat = solve(t(X) %*% X) %*% t(X) %*% sim_data$y
    y_hat_xo[i] = xo %*% beta_hat

  }
  data.frame(beta_hat_1 = beta_hat_1, se_squared = se_squared, y_hat_xo = y_hat_xo)
}

#run 3000 simulations for 3 different values of sigma, generating 3 different dataframes
sim1 = simulate(num_sims, sigma_1)
sim2 = simulate(num_sims, sigma_2)
sim3 = simulate(num_sims, sigma_3)




```

###Results
```{r}
#Examine beta_1 for simulations

get_true_variance = function(sigma){
  sigma ^ 2 * C[1 + 1, 1 + 1]
}

get_true_sd = function(sigma){
  sqrt(sigma ^ 2 * C[1 + 1, 1 + 1])
}

true_mean      = c(beta_1,beta_1, beta_1)
empirical_mean = c(mean(sim1$beta_hat_1),      mean(sim2$beta_hat_1),      mean(sim3$beta_hat_1))
true_variance  = c(get_true_variance(sigma_1), get_true_variance(sigma_2), get_true_variance(sigma_3))
empirical_var  = c(var(sim1$beta_hat_1),       var(sim2$beta_hat_1),       var(sim3$beta_hat_1))
true_sd        = c(get_true_sd(sigma_1),       get_true_sd(sigma_2),       get_true_sd(sigma_3))
empirical_sd   = c(sd(sim1$beta_hat_1),        sd(sim2$beta_hat_1),        sd(sim3$beta_hat_1))

simulations    = c("sim 1", "sim 2", "sim 3")
study_1_sigmas = c(sigma_1, sigma_2, sigma_3)

results = data.frame(simulations, study_1_sigmas, true_mean, empirical_mean, true_variance, empirical_var, true_sd, empirical_sd)
colnames(results) = c("Simulation", "Sigma Value", "True Mean", "Empirical Mean", "True Variance", "Empirical Variance", "True SD", "Empirical SD" )

```

###1.1.1 Table for analysis

```{r}
knitr::kable(results, caption = "Analysis for True and Empirical Values Associated with beta_1_hat")
```

###1.1.2 Examine Distribution
```{r}
#examine distibu
prob = rep(0, 3)
get_prob = function(sigma, beta_hat_1){
  sd_bh1 = sqrt(sigma ^ 2 * C[1 + 1, 1 + 1])
  prob[1] = mean(beta_1 - 1 * sd_bh1 < beta_hat_1 & beta_hat_1 < beta_1 + 1 * sd_bh1)
  prob[2] = mean(beta_1 - 2 * sd_bh1 < beta_hat_1 & beta_hat_1 < beta_2 + 2 * sd_bh1)
  prob[3] = mean(beta_1 - 3 * sd_bh1 < beta_hat_1 & beta_hat_1 < beta_2 + 3 * sd_bh1)
  prob
}
# We expect these to be: 0.68, 0.95, 0.997 since beta_hat_1 follows a normal distribution
#Get Probability for simulation 1
get_prob(sigma_1, sim1$beta_hat_1)
#Get Probability for simulation 2
get_prob(sigma_2, sim2$beta_hat_1)
#Get Probability for simulation 3
get_prob(sigma_3, sim3$beta_hat_1)
```

###1.1.3 Histogram for $\hat{\beta}_1$ for $\sigma$ values of 1, 5, 10
```{r fig.height=20, fig.width=10}
#setting to a 3x1
par(mfrow = c(3,1))

#simulation study with sigma = 1, normal distibution
hist(sim1$beta_hat_1, prob = TRUE, breaks = 20,
     xlab = expression("Histogram for" ~ hat(beta)[1]), main = expression(hat(beta)[1]  ~ "for" ~ sigma ~ "= 1") , border = "dodgerblue", cex = 1.5)
curve(dnorm(x, mean = beta_1, sd = sqrt(sigma_1 ^ 2 * C[1 + 1, 1 + 1])),
      col = "darkorange", add = TRUE, lwd = 3)
legend("topright", "True Distribution", lty = c(1, 2), lwd = 2, col = "darkorange")


#simulation study with sigma = 5, normal distibution
hist(sim2$beta_hat_1, prob = TRUE, breaks = 20,
     xlab = expression("Histogram for" ~ hat(beta)[1]), main = expression(hat(beta)[1]  ~ "for" ~ sigma ~ "= 5") , border = "dodgerblue", cex = 1.5)
curve(dnorm(x, mean = beta_1, sd = sqrt(sigma_2 ^ 2 * C[1 + 1, 1 + 1])),
      col = "darkorange", add = TRUE, lwd = 3)
legend("topright", "True Distribution", lty = c(1, 2), lwd = 2, col = "darkorange")


#simulation study with sigma = 10, normal distibution
hist(sim3$beta_hat_1, prob = TRUE, breaks = 20,
     xlab = expression("Histogram for" ~ hat(beta)[1]), main = expression(hat(beta)[1]  ~ "for" ~ sigma ~ "= 10") , border = "dodgerblue", cex = 1.5)
curve(dnorm(x, mean = beta_1, sd = sqrt(sigma_3 ^ 2 * C[1 + 1, 1 + 1])),
      col = "darkorange", add = TRUE, lwd = 3)
legend("topright", "True Distribution", lty = c(1, 2), lwd = 2, col = "darkorange")
dev.off()
```

###1.1.4 Plot of true normal distribution of $\hat{\beta_1}$
```{r}
# plot curve for true normal distributions of beta_1_hat
x = seq(-4, 6, length = 100)
plot(x, dnorm(x, mean = 1, sd = sqrt(sigma_1 ^ 2 * C[1 + 1, 1 + 1])), type = "l", lty = 1, lwd = 2,
     xlab = expression(hat(beta)[1]), ylab = "Density", main = expression("True normal distribution of " ~ hat(beta)[1] ~ "for" ~ sigma ~ "values"), col = "darkorange")

curve(dnorm(x, mean = beta_1, sd = sqrt(sigma_2 ^ 2 * C[1 + 1, 1 + 1])),
      col = "dodgerblue", add = TRUE, lwd = 3)


curve(dnorm(x, mean = beta_1, sd = sqrt(sigma_3 ^ 2 * C[1 + 1, 1 + 1])),
      col = "red", add = TRUE, lwd = 3)

legend("topright", c(expression(sigma ~ " = 1"), expression(sigma ~ " = 5"), expression(sigma ~ " = 10")), lty = c(1,1,1), lwd = 2,
       col = c("darkorange", "dodgerblue", "red"), adj = c(0.2, 0.6), cex=0.75)

```

###1.2.1 Table for analysis $\hat{E}[y(x_o)]$

```{r}
#please not that for the given x0, in code I am useing xoo to differentiate this value from the x0 found in the X matrix
#as with y_hat_xo as well

C = solve(t(X) %*% X)
C
xoo = c(1, -3, 2.5, .5, 0)

#comparing means
true_mean      = c(2,2, 2)
empirical_mean = c(mean(sim1$y_hat_xo),      mean(sim2$y_hat_xo),      mean(sim3$y_hat_xo))

true_variance  = c((sigma_1 ^2 * ( t(xoo) %*% C %*% xoo )), sigma_2 ^ 2 * ( t(xoo) %*% C %*% xoo ), sigma_3 ^ 2 * ( t(xoo) %*% C %*% xoo ))
empirical_var  = c(var(sim1$y_hat_xo),       var(sim2$y_hat_xo),       var(sim3$y_hat_xo))

true_sd  = c((sigma_1 * sqrt( t(xoo) %*% C %*% xoo )), sigma_2 * sqrt( t(xoo) %*% C %*% xoo ), sigma_3 * sqrt( t(xoo) %*% C %*% xoo ))
empirical_sd  = c(sd(sim1$y_hat_xo),       sd(sim2$y_hat_xo),       sd(sim3$y_hat_xo))

results = data.frame(simulations, study_1_sigmas, true_mean, empirical_mean, true_variance, empirical_var, true_sd, empirical_sd)
colnames(results) = c("Simulation", "Sigma Value", "True Mean", "Empirical Mean", "True Variance", "Empirical Variance", "True SD", "Empirical SD" )

knitr::kable(results, caption = "Analysis for True and Empirical Values $\\hat{\\text{E}}[Y \\mid x_1 = -3, x_2 = 2.5, x_3 = 0.5, x_4 = 0]")


```

###1.2.2 Examine Distribution of 1.2.1 $\hat{E}[y(x_o)]$
```{r}
prob = rep(0, 3)
get_prob_y_hat_xo = function(sigma, y_hat_xo){
  sd_bh1 = as.vector(sigma * sqrt( t(xoo) %*% C %*% xoo ))
  prob[1] = mean(2 - 1 * sd_bh1 < y_hat_xo & y_hat_xo < 2 + 1 * sd_bh1)
  prob[2] = mean(2 - 2 * sd_bh1 < y_hat_xo & y_hat_xo < 2 + 2 * sd_bh1)
  prob[3] = mean(2 - 3 * sd_bh1 < y_hat_xo & y_hat_xo < 2 + 3 * sd_bh1)
  prob
}
# We expect these to be: 0.68, 0.95, 0.997 since beta_hat_1 follows a normal distribution
#Get Probability for simulation 1
get_prob_y_hat_xo(sigma_1, sim1$y_hat_xo)
get_prob_y_hat_xo(sigma_2, sim2$y_hat_xo)
get_prob_y_hat_xo(sigma_3, sim3$y_hat_xo)
```

##1.2.3 Examine Histogram for $\hat{E}[y(x_o)]$
```{r fig.height=20, fig.width=10}
#setting to a 3x1
par(mfrow = c(3,1))
hist(sim1$y_hat_xo, prob = TRUE, breaks = 20,
     xlab = expression(hat(E) ~ "[Y|" ~ x[1] ~ "= -3," ~ x[2] ~" = 2.5," ~ x[3] ~" = 0.5," ~ x[4] ~ "= 0]"), main = expression("Histogram for" ~ hat(E) ~ "[Y] when " ~ sigma ~ " = 1"), border = "dodgerblue",  ylim = c(0, .2), cex = 1.5)

                
curve(dnorm(x, mean = 2, sd = sigma_1 * sqrt( t(xo) %*% C %*% xo)),
      col = "darkorange", add = TRUE, lwd = 3)

#simulation 2
hist(sim2$y_hat_xo, prob = TRUE, breaks = 20,
     xlab = expression(hat(E) ~ "[Y|" ~ x[1] ~ "= -3," ~ x[2] ~" = 2.5," ~ x[3] ~" = 0.5," ~ x[4] ~ "= 0]"), main = expression("Histogram for" ~ hat(E) ~ "[Y] when " ~ sigma ~ " = 5"), border = "dodgerblue",  ylim = c(0, .04), cex = 1.5)


curve(dnorm(x, mean = 2, sd = sigma_2 * sqrt( t(xo) %*% C %*% xo)),
      col = "darkorange", add = TRUE, lwd = 3)

#simulation 3
hist(sim3$y_hat_xo, prob = TRUE, breaks = 20,
     xlab = expression(hat(E) ~ "[Y|" ~ x[1] ~ "= -3," ~ x[2] ~" = 2.5," ~ x[3] ~" = 0.5," ~ x[4] ~ "= 0]"), main = expression("Histogram for" ~ hat(E) ~ "[Y] when " ~ sigma ~ " = 10"), border = "dodgerblue",  ylim = c(0, .02), cex = 1.5)


curve(dnorm(x, mean = 2, sd = sigma_3 * sqrt( t(xo) %*% C %*% xo)),
      col = "darkorange", add = TRUE, lwd = 3)
```

##1.2.3 Examine Histogram for $s_e^2$
```{r fig.height=20, fig.width=10}
#setting to a 3x1
par(mfrow = c(3,1))
#simulation 1
hist(((n-p) * sim1$se_squared)/(sigma_1^2), prob = TRUE, breaks = 20,
     xlab = mtext((expression( over((n-p) * se ^ 2, sigma ^ 2))), side=1, line=4), main = expression("Histogram for" ~ over((n-p) * se ^ 2, sigma ^ 2 ) ~ "where n = 15, p = 5," ~ sigma ~ "= 1"), border = "dodgerblue", cex = 1.5)

curve(dchisq(x, df = 10),
      col = "darkorange", add = TRUE, lwd = 3)
legend("topright", "True Distribution", lty = c(1, 2), lwd = 2, col = "darkorange")

#simulation 2
hist(((n-p) * sim2$se_squared)/(sigma_2^2), prob = TRUE, breaks = 20,
     xlab = mtext((expression( over((n-p) * se ^ 2, sigma ^ 2))), side=1, line=4), main = expression("Histogram for" ~ over((n-p) * se ^ 2, sigma ^ 2 ) ~ "where n = 15, p = 5," ~ sigma ~ "= 1"), border = "dodgerblue", cex = 1.5)

curve(dchisq(x, df = 10),
      col = "darkorange", add = TRUE, lwd = 3)
legend("topright", "True Distribution", lty = c(1, 2), lwd = 2, col = "darkorange")

#simulation 3
hist(((n-p) * sim3$se_squared)/(sigma_3^2), prob = TRUE, breaks = 20,
     xlab = mtext((expression( over((n-p) * se ^ 2, sigma ^ 2))), side=1, line=4), main = expression("Histogram for" ~ over((n-p) * se ^ 2, sigma ^ 2 ) ~ "where n = 15, p = 5," ~ sigma ~ "= 1"), border = "dodgerblue", cex = 1.5)

curve(dchisq(x, df = 10),
      col = "darkorange", add = TRUE, lwd = 3)
legend("topright", "True Distribution", lty = c(1, 2), lwd = 2, col = "darkorange")

dev.off()
```

```{r}

```

###Discussion

####Discussion of  $\hat{\beta}_1$

For the discussion we will first examine $\hat{\beta}_1$.  The following information will lead to understanding the true distribution for $\hat{\beta}_1$ has been determined to be:

$\hat{\beta}_1 \sim N(\beta_1, \sigma^2  C_{22}) = N(1, \sigma^2 * 0.02858036)$
where
$C = \left(X^\top X\right)^{-1}$

**Examining Empirical Mean vs Expected value for $\beta_1$**

To determine this, please refer to the table [Table for analysis 1.1] in the results section which shows for each of the values of sigma, over 3000 simulations, the empirical mean is very close to the true mean. (Note that as the value of **sigma** increases, the difference between the true mean and the empirical mean increases).  The empirical mean was calculated by taking the mean of the beta_hat_1 values that were calculated, in a normal distribution we know E[$\hat{\beta_1}$] = $\beta_1$, so this is evidence towards this following a normal distribution.

**Examining Empical Variance vs Variance expected from the normal distribution**

Not only is the mean of the simulated values for each value of sigma very close to the true value of $\beta_1$ (as expected), but the empirical variance is very close to the variance associated with a normal distribution for a multiple linear regression.  In an MLR (multiple linear regression) model, the variance of $\hat{\beta_1}$ is $\sigma^2 * C_{jj}$, given we are looking at $\hat{\beta_1}$, that would mean looking at $C_{22}$, which is equal to 0.02858036.  As the table [Table for analysis 1.1] shows, the values for the empirical variance of $\hat{\beta_1}$ are very close to the true variance of $\beta_1$.

**Examining Probabilities following a normal distribution**

After examining the mean and variance, looking at section [Examine Distribution], you can see the calculated proportion of sample means within 1, 2, and 3 standard deviations of the population mean.  These values were expected to be close to 0.68, 0.95, and 0.997.  For each of the values of $\sigma$, this was true, thus showing that the true distribution fo the estimate $\hat{\beta}_1$  follows a normal distribution.

**Plotting the sample distributions along with the expected normal distribution**
Then in plotting the sample shown in section [Histogram for $\hat{\beta}_1$ for $\sigma$ values of 1, 5, 10], we can see that the distribution of the simulated values does appear to follow a normal distribution.  Through this information we can make the claim that the true distribution of $\hat{\beta}_1$ is:

$\hat{\beta}_1 \sim N(\beta_1, \sigma^2  C_{22}) = N(1, \sigma^2 * 0.02858036)$

**Changes in the distribution with different values of $sigma$**

In the normal distribution, $sigma$ will impact the variance associated with the distribution.  An increase in $sigma$ will increase the variance, this is shown the in the plot [Plot of true normal distribution of $\hat{\beta_1}$]

####Discussion of $\hat{\text{E}}[Y \mid x_1 = -3, x_2 = 2.5, x_3 = 0.5, x_4 = 0]$

The following information will lead to an understanding that the true distribution for $\hat{\text{E}}[Y \mid x_1 = -3, x_2 = 2.5, x_3 = 0.5, x_4 = 0]$ has been determined to be:

$\hat{\text{E}}[Y \mid x_1 = -3, x_2 = 2.5, x_3 = 0.5, x_4 = 0] \sim N(\hat{y}(x_o), \sigma^2 * x_o^\top \left(X^\top X\right)^{-1} x_o)$


$\hat{\text{E}}[Y \mid x_1 = -3, x_2 = 2.5, x_3 = 0.5, x_4 = 0] \sim N(2, \sigma^2 * 4.154206)$


**Examining Empirical Mean vs Expected value for $\hat{\text{E}}[Y \mid x_1 = -3, x_2 = 2.5, x_3 = 0.5, x_4 = 0]$.**

To determine this please refer to the table [Table for analysis 1.2] in the results section which shows for each of the values of sigma, over 3000 simulations, the empirical mean is very close to the true mean.  (Note that as the value of ???? increases, the difference between the true mean and the empirical mean increases).  The empirical mean was calculated by taking the mean of the y_hat_xo values that were calculated during the simulations, in a normal distribution we know that E[$\hat{\text{E}}[Y \mid x_1 = -3, x_2 = 2.5, x_3 = 0.5, x_4 = 0]$] = $\beta_0 + \beta_1 (-3) + \beta_2 (2.5) + \beta_3(0.5)  + \beta_4 (0)$ = $2 + (-3) +  (2.5) + (0.5)  + (0)$ = 2, so this is evidence towards following a normal distribution.

**Examining Empical Variance vs Variance expected from the normal distribution**

Not only is the mean of the simulated values for each of the sigma values very close to the true value (as expected), but the empirical variance is very close to the variance associated with the normal distribution.  Please look at the table [Table for analysis 1.2] to see the true variance is very close to the empirical variance.  

Note that the $Var[\hat{y}(x_o)]$ = $\sigma ^ 2 * x_o^\top \left(X^\top X\right)^{-1} x_o$ which has been calculated in table [Table for analysis 1.2] showing very close values.  

**Examining Probabilities following a normal distribution**

As section [Examine Distribution of y_hat_xo] shows, the probabilies are following a normal distribution as expected, for standard deviations of 1, 2 and 3 are very close to 0.68, 0.95, and 0.997.

**Plotting Historgram of**

Plotting the results of the simulations for $\hat{\text{E}}[Y \mid x_1 = -3, x_2 = 2.5, x_3 = 0.5, x_4 = 0]$ with the normal distribution shown in section [Examine Histogram for y_hat_xo] is further evidence that this does in fact follow a normal distribution of:

$\hat{\text{E}}[Y \mid x_1 = -3, x_2 = 2.5, x_3 = 0.5, x_4 = 0] \sim N(2, \sigma^2 * 4.154206)$

Again, as the value of $\sigma$ increases, the variance found in the distribution increases.

####Discussion of  $s_e^2$



## Simulation Study 2

###Introduction
The intent of this simulation study is to explore how well the TEST RMSE will select the correct model.  During this simulation study a dataset will be randomly split into 2 sets of equal sizes (300 observations for training, 300 observations for testing).  between a train dataset and a test dataset evenly over 500 simulations.  The dataset used for this simulation comes from data stored in [`study_2.csv](study_2.csv) which contains values of the 9 predictors with a sample size of 600.  

Duruing each of the 500 simulations, a total of 9 models will be fit, and the Train and Test RMSE will be calculated for 3 different values of $\sigma$.

$$\text{RMSE}(\text{model, data}) = \sqrt{\frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}$$

###Methods

```{r}
#set seed
birthday = 19810803
set.seed(birthday)

#set known model parameters
beta_0 = 0
beta_1 = 6
beta_2 = -3.5
beta_3 = 1.7
beta_4 = -1.1
beta_5 = 0.7
#set values of sigma for each simulation
sigma_1 = 1
sigma_2 = 2
sigma_3 = 4

num_sims = 500
sample_size_split = 300
sample_size_total = 600

library(readr)
study_2 = read_csv("study_2.csv")


x0 = rep(1, sample_size_total)
x1 = study_2$x1
x2 = study_2$x2
x3 = study_2$x3
x4 = study_2$x4
x5 = study_2$x5
x6 = study_2$x6
x7 = study_2$x7
x8 = study_2$x8
x9 = study_2$x9
y = rep(0, sample_size_total)


sim_data = data.frame(x1, x2, x3, x4, x5,x6, x7, x8, x9,  y)

get_predict = function(model, test_data){
  test_data$y - predict(model, newdata = test_data)
}

get_RMSE= function(data) {
  RMSE   = sqrt( mean(data ^ 2  ))
}

simulate2 = function(num_sims, sigma){
  RMSE_train   = matrix(0, nrow = num_sims, ncol = 9)
  RMSE_test    =  matrix(0, nrow = num_sims, ncol = 9)
  winner_test  = rep(0, num_sims)
  winner_train = rep(0, num_sims)
  
  for(i in 1:num_sims) {
      
    eps = rnorm(sample_size_total, mean = 0 , sd = sigma)
    
    sim_data$y = beta_0 * x0 + beta_1 * x1 + beta_2 * x2 + beta_3 * x3 + beta_4 * x4 + beta_5 * x5 +   eps
    
    train_index = sample(1:nrow(sim_data), sample_size_split)
    train = sim_data[train_index ,]
    test  = sim_data[-train_index ,]
    
    model_1 = lm( y ~ x1, data = train)
    model_2 = lm( y ~ x1 + x2, data = train)
    model_3 = lm( y ~ x1 + x2 + x3, data = train)
    model_4 = lm( y ~ x1 + x2 + x3 + x4, data = train)
    model_5 = lm( y ~ x1 + x2 + x3 + x4 + x5, data = train)
    model_6 = lm( y ~ x1 + x2 + x3 + x4 + x5 + x6, data = train)
    model_7 = lm( y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7, data = train)
    model_8 = lm( y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = train)
    model_9 = lm( y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9, data = train)
    
    RMSE_train[i ,] = c(get_RMSE(resid(model_1)), get_RMSE(resid(model_2)), get_RMSE(resid(model_3)), get_RMSE(resid(model_4)),
                      get_RMSE(resid(model_5)), get_RMSE(resid(model_6)), get_RMSE(resid(model_7)), get_RMSE(resid(model_8)),
                      get_RMSE(resid(model_9))
                      )

    RMSE_test[i, ] = c(get_RMSE(get_predict(model_1, test)), get_RMSE(get_predict(model_2, test)), 
                       get_RMSE(get_predict(model_3, test)), get_RMSE(get_predict(model_4, test)), 
                       get_RMSE(get_predict(model_5, test)), get_RMSE(get_predict(model_6, test)), 
                       get_RMSE(get_predict(model_7, test)), get_RMSE(get_predict(model_8, test)), 
                       get_RMSE(get_predict(model_9, test))
                       )
    winner_test[i] = which.min(RMSE_test[i, ])
    winner_train[i] = which.min(RMSE_train[i, ])
    
  }
  result = data.frame(RMSE_train, RMSE_test, winner_test, winner_train) 
  colnames(result) <- c( "TrainRMSE_M1", "TrainRMSE_M2", "TrainRMSE_M3", "TrainRMSE_M4", "TrainRMSE_M5", "TrainRMSE_M6", 
                         "TrainRMSE_M7", "TrainRMSE_M8", "TrainRMSE_M9", 
                        "TestRMSE_M1", "TestRMSE_M2", "TestRMSE_M3", "TestRMSE_M4", "TestRMSE_M5", "TestRMSE_M6", "TestRMSE_M7", 
                        "TestRMSE_M8", "TestRMSE_M9", "TestWinner", "TrainWinner")
  result
}

#run simulations 500 times for values of sigma: 1, 2, 4
sim1_sigma1 = simulate2(500, 1)
sim2_sigma2 = simulate2(500, 2)
sim3_sigma3 = simulate2(500, 4)


get_means = function(data){
  end = ncol(data) -2
  model_means = rep(0, end)
  for(i in 1:end) {
    model_means[i] = mean(sim1_sigma1[, i])
  }
  model_means
}

sim1_means = get_means(sim1_sigma1)
sim2_means = get_means(sim2_sigma2)
sim3_means = get_means(sim3_sigma3)



```


###Results
```{r fig.height=20, fig.width=10}
#setting to a 3x1
par(mfrow = c(3,2))

plot.new()

hist(sim1_sigma1$TestWinner, 
     main   = "Simulation Model Selected based on Test Data",
     xlab   = "Number of predictors in model",
     col    = "dodgerblue",
     border = "darkorange", axis(side=1, at=seq(0,10, 1)))

hist(sim1_sigma1$TrainWinner,
     main   = "Simulation Model Selected based on Train Data",
     xlab   = "Number of predictors in model",
     col    = "dodgerblue",
     border = "darkorange", axis(side=1, at=seq(0,10, 1)))

hist(sim2_sigma2$TestWinner, 
     main   = "Simulation Model Selected based on Test Data",
     xlab   = "Number of predictors in model",
     col    = "dodgerblue",
     border = "darkorange", axis(side=1, at=seq(0,10, 1)))

hist(sim2_sigma2$TrainWinner,
     main   = "Simulation Model Selected based on Train Data",
     xlab   = "Number of predictors in model",
     col    = "dodgerblue",
     border = "darkorange", axis(side=1, at=seq(0,10, 1)))

hist(sim3_sigma3$TestWinner, 
     main   = "Simulation Model Selected based on Test Data",
     xlab   = "Number of predictors in model",
     col    = "dodgerblue",
     border = "darkorange", axis(side=1, at=seq(0,10, 1)))

hist(sim3_sigma3$TrainWinner,
     main   = "Simulation Model Selected based on Train Data",
     xlab   = "Number of predictors in model",
     col    = "dodgerblue",
     border = "darkorange", axis(side=1, at=seq(0,10, 1)))
dev.off()
```

```{r}
par(mfrow = c(1,3))
#plot for sigma = 1 RMSE Mean 
plot(sim1_means[1:9], pch = 20, col = "dodgerblue", cex = 1,
     xlab = "Mean RMSE Response for a model with x predictors",
     ylab = "RMSE",
     main = "Simulated Regression Data")
points(sim1_means[10:18], pch = 20, col = "darkorange", cex = 1)
lines(sim1_means[10:18], col = "darkorange")
lines(sim1_means[1:9], col = "dodgerblue")

#plot for sigma = 2 RMSE Mean 
plot(sim2_means[1:9], pch = 20, col = "dodgerblue", cex = 1,
     xlab = "Mean RMSE Response for a model with x predictors",
     ylab = "RMSE",
     main = "Simulated Regression Data")
points(sim2_means[10:18], pch = 20, col = "darkorange", cex = 1)
lines(sim2_means[10:18], col = "darkorange")
lines(sim2_means[1:9], col = "dodgerblue")


#plot for sigma = 4 RMSE Mean 
plot(sim3_means[1:9], pch = 20, col = "dodgerblue", cex = 1,
     xlab = "Mean RMSE Response for a model with x predictors",
     ylab = "RMSE",
     main = "Simulated Regression Data")
points(sim3_means[10:18], pch = 20, col = "darkorange", cex = 1)
lines(sim3_means[10:18], col = "darkorange")
lines(sim3_means[1:9], col = "dodgerblue")

dev.off()
```


###Discussion
Does this method **always** select the correct model?  On average does it select the correct model.  How does noise affect the results


## Simulation Study 1

###Introduction

###Methods

```{r}
birthday = 19810803
set.seed(birthday)

num_sims = 1000 # number of simulations
x_values = seq(0, 5, length = 25)
n        = length(x_values)   # sample size
alpha    = c(0.01,0.05,0.10)
beta_1   = seq(0, 3, .1)
p_value  = rep(0, length(beta_1))


simulate3 = function(num_sim, sigma){
  p_values = matrix(0, num_sim,  length(beta_1))  
  for(sim in 1: num_sim){
    for(i in 1:length(beta_1)) {
      signal   = x_values * beta_1[i]
      eps      = rnorm(25, mean= 0, sd = sigma)
      y        = signal + eps
      fit      = lm(y ~ x_values)
      p_value[i] = summary(fit)$coefficients[2,4]
    }
    p_values[sim, ]  = p_value
  }
  as.data.frame(p_values)
}

get_power = function(data, alpha){
  pwr = matrix(0, nrow = length(alpha), ncol = ncol(data))
  for (j in 1: length(alpha)){
    rejectHo = rep(0, ncol(data))
    for (i in 1: ncol(data)){
      #we should reject Ho when beta > 0
      rejectHo[i] = sum(data[ , i] < alpha[j])
    }
    pwr[ j ,  ] = rejectHo/nrow(data)  
  }
  pwr = as.data.frame(pwr)
  colnames(pwr) = beta_1
  pwr
}

#simulate for sigma = 1
data_sigma1  = simulate3(1000, 1)
#get power power = #tests rejected/#simulations for a given beta_1 value
power_sigma1 = get_power(data_sigma1, alpha)

#simulate for sigma = 2
data_sigma2  = simulate3(1000, 2)
#get power power = #tests rejected/#simulations for a given beta_1 value
power_sigma2 = get_power(data_sigma2, alpha)

#simulate for sigma = 4
data_sigma3  = simulate3(1000, 4)
#get power power = #tests rejected/#simulations for a given beta_1 value
power_sigma3 = get_power(data_sigma3, alpha)
```



###Results

```{r fig.height=20, fig.width=10}
#setting to a 3x1
par(mfrow = c(3,1))

#Sigma = 1
par(mar = c(5, 5, 2, 2)) # adjusted plot margins, otherwise the "hat" does not display

plot(beta_1, power_sigma1[1, ], pch = 20, col = "dodgerblue", cex = 1,
     xlab = expression(hat(beta)[1] ~ "values"),
     ylab = expression(hat(Power)),
     main = expression(hat(Power) ~ "for " ~ sigma ~ " = 1"))

points(beta_1, power_sigma1[2, ], pch = 20, col = "darkorange", cex = 1)

points(beta_1, power_sigma1[3, ], pch = 20, col = "red", cex = 1)

lines(beta_1, power_sigma1[1, ], col = "dodgerblue")

lines(beta_1, power_sigma1[2, ], col = "darkorange")

lines(beta_1, power_sigma1[3, ], col = "red")

legend("bottomright", c(expression(alpha ~ " = 0.01" ), expression(alpha ~ "= 0.05"), expression(alpha ~ "= 0.10")), lty = 1, lwd = 2,
       col = c("dodgerblue", "darkorange", "red"), adj = c(0.2, 0.6))

#Sigma = 2

par(mar = c(5, 5, 2, 2)) # adjusted plot margins, otherwise the "hat" does not display

plot(beta_1, power_sigma2[1, ], pch = 20, col = "dodgerblue", cex = 1,
     xlab = expression(hat(beta)[1] ~ "values"),
     ylab = expression(hat(Power)),
     main = expression(hat(Power) ~ "for " ~ sigma ~ " = 2"))

points(beta_1, power_sigma2[2, ], pch = 20, col = "darkorange", cex = 1)

points(beta_1, power_sigma2[3, ], pch = 20, col = "red", cex = 1)

lines(beta_1, power_sigma2[1, ], col = "dodgerblue")

lines(beta_1, power_sigma2[2, ], col = "darkorange")

lines(beta_1, power_sigma2[3, ], col = "red")

legend("bottomright", c(expression(alpha ~ " = 0.01" ), expression(alpha ~ "= 0.05"), expression(alpha ~ "= 0.10")), lty = 1, lwd = 2,
       col = c("dodgerblue", "darkorange", "red"), adj = c(0.2, 0.6))


#Sigma = 3

par(mar = c(5, 5, 2, 2)) # adjusted plot margins, otherwise the "hat" does not display

plot(beta_1, power_sigma3[1, ], pch = 20, col = "dodgerblue", cex = 1,
     xlab = expression(hat(beta)[1] ~ "values"),
     ylab = expression(hat(Power)),
     main = expression(hat(Power) ~ "for " ~ sigma ~ " = 4"))

points(beta_1, power_sigma3[2, ], pch = 20, col = "darkorange", cex = 1)

points(beta_1, power_sigma3[3, ], pch = 20, col = "red", cex = 1)

lines(beta_1, power_sigma3[1, ], col = "dodgerblue")

lines(beta_1, power_sigma3[2, ], col = "darkorange")

lines(beta_1, power_sigma3[3, ], col = "red")

legend("bottomright", c(expression(alpha ~ " = 0.01" ), expression(alpha ~ "= 0.05"), expression(alpha ~ "= 0.10")), lty = 1, lwd = 2,
       col = c("dodgerblue", "darkorange", "red"), adj = c(0.2, 0.6))
```


###Discussion

